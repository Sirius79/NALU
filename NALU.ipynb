{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NALU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Sirius79/NALU/blob/master/NALU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "33wXW9yBfPzl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n",
        "!pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1L3cDHMd5ej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import  torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ig_PpRQqe6RR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NACCell(nn.Module):\n",
        "  '''\n",
        "    Neural Accumulator Cell\n",
        "  '''\n",
        "  def __init__(self, input_size, output_size):\n",
        "    '''\n",
        "      input_size: input dimension\n",
        "      output_size: output dimension\n",
        "    '''\n",
        "    super(NACCell, self).__init__()\n",
        "    self.input = input_size\n",
        "    self.output = output_size\n",
        "    \n",
        "    self.W_h = Parameter(torch.Tensor(self.output, self.input, device=device))\n",
        "    self.M_h = Parameter(torch.Tensor(self.output, self.input, device=device))\n",
        "    self.W = Parameter(torch.tanh(self.W_h) * torch.sigmoid(self.M_h))\n",
        "    self.register_parameter('bias', None)\n",
        "    \n",
        "    init.xavier_uniform_(self.W_h)\n",
        "    init.xavier_uniform_(self.M_h)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    return F.linear(input, self.W, self.bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cpHyn1ElI7gs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NAC(nn.Module):\n",
        "  '''\n",
        "    stack Neural Accumulator Cells\n",
        "  '''\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "    '''\n",
        "      input_size: input dim\n",
        "      hidden_size: hidden dim\n",
        "      output_size: output dim\n",
        "      num_layers: number of layers\n",
        "    '''\n",
        "    super(NAC, self).__init__()\n",
        "    self.input = input_size\n",
        "    self.hidden = hidden_size\n",
        "    self.output = output_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    layers = [NACCell(self.hidden if i>0 else self.input, self.hidden if i<num_layers-1 else self.output) for i in range(num_layers)]\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dBuFuaUuupL-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralALUCell(nn.Module):\n",
        "  '''\n",
        "    Neural Arithmetic Logic Unit Cell \n",
        "  '''\n",
        "  def __init__(self, input_size, output_size, epsilon = 1e-5):\n",
        "    '''\n",
        "      input_size: input dimension\n",
        "      output_size: output dimension\n",
        "      epsilon: prevents log0\n",
        "    '''\n",
        "    super(NeuralALUCell, self).__init__()\n",
        "    self.input = input_size\n",
        "    self.output = output_size\n",
        "    self.epsilon = epsilon\n",
        "    \n",
        "    self.G = Parameter(torch.Tensor(self.output, self.input, device=device))\n",
        "    self.W = Parameter(torch.Tensor(self.output, self.input, device=device))\n",
        "    self.register_parameter('bias', None)\n",
        "    self.nac = NACCell(self.input, self.output)\n",
        "    \n",
        "    init.xavier_uniform_(self.G)\n",
        "    init.xavier_uniform_(self.W)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    '''\n",
        "      g*a: addition-subtraction\n",
        "      (1-g)*m: multiplication-division\n",
        "    '''\n",
        "    a = self.nac(input)\n",
        "    g = torch.sigmoid(F.linear(input, self.G, self.bias))\n",
        "    m = torch.exp(F.linear(torch.log(torch.abs(input) + self.epsilon), self.W, self.bias))\n",
        "    return g*a + (1-g)*m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YBZkIYYmHDBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NALU(nn.Module):\n",
        "  '''\n",
        "    stacked NALU Cells\n",
        "  '''\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers, epsilon = 1e-5):\n",
        "    '''\n",
        "      input_size: input dim\n",
        "      hidden_size: hidden dim\n",
        "      output_size: output dim\n",
        "      num_layers: number of layers\n",
        "      epsilon: prevents log0\n",
        "    '''\n",
        "    super(NALU, self).__init__()\n",
        "    self.input = input_size\n",
        "    self.hidden = hidden_size\n",
        "    self.output = output_size\n",
        "    self.num_layers = num_layers\n",
        "    self.epsilon = epsilon\n",
        "    \n",
        "    layers = [NeuralALUCell(self.hidden if i>0 else self.input, self.hidden if i<num_layers-1 else self.output, self.epsilon) for i in range(num_layers)]\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EDYT1jRF4L-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple Learning Functions"
      ]
    },
    {
      "metadata": {
        "id": "-9nmW4cWbx8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f13e24e2-0539-4195-b700-107516cbff93"
      },
      "cell_type": "code",
      "source": [
        "fn = lambda x, y: x * y\n",
        "x = torch.rand(2)*10\n",
        "y = torch.tensor([fn(*x)])\n",
        "print(x, y)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3.1639, 6.5570]) tensor([20.7458])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "467SZxv0VZ7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(train_size, test_size, fn):\n",
        "  X, Y = torch.Tensor(train_size + test_size, 2, device=device), torch.Tensor(train_size + test_size, 1, device=device)\n",
        "  for i in range(train_size + test_size):\n",
        "    x = torch.rand(2, device=device)*10\n",
        "    y = torch.tensor([fn(*x)], device=device)\n",
        "    X[i] = x\n",
        "    Y[i] = y\n",
        "  X_train, y_train = X[:train_size], Y[:train_size]\n",
        "  X_test, y_test = X[train_size:], Y[train_size:]\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWuDG4eQdwUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fn = lambda x, y: x % y\n",
        "#X, Y = generate(500, 50, fn)\n",
        "X_tr, y_tr, X_t, y_t = generate(500, 50, fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XgI4ibvrehZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d8162c7-5a39-4a9f-979b-65dd83ceb12f"
      },
      "cell_type": "code",
      "source": [
        "print(X_tr[0],y_tr[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([8.3774, 3.9029]) tensor([0.5716])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SYcYcPQxgN-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, data, target, num_iters):\n",
        "  all_losses = []\n",
        "  for i in range(num_iters):\n",
        "    out = model(data)\n",
        "    loss = F.mse_loss(out, target)\n",
        "    all_losses.append(loss)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 1000 == 0:\n",
        "      print(\"\\t{}/{}: loss: {:.7f} \".format(i+1, num_iters, loss.item()))\n",
        "  return all_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQpp90gZjfQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1830
        },
        "outputId": "14809c47-7c53-4e6d-8f79-440a4f218728"
      },
      "cell_type": "code",
      "source": [
        "net = NALU(2,2,1,2)\n",
        "optim = torch.optim.RMSprop(net.parameters(), lr=1e-3)\n",
        "all_losses = train(net, optim, X_tr, y_tr, 100000)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t1/100000: loss: 4.2592731 \n",
            "\t1001/100000: loss: 1.6628846 \n",
            "\t2001/100000: loss: 1.5179950 \n",
            "\t3001/100000: loss: 1.2823309 \n",
            "\t4001/100000: loss: 1.0541124 \n",
            "\t5001/100000: loss: 0.8825064 \n",
            "\t6001/100000: loss: 0.7961071 \n",
            "\t7001/100000: loss: 0.7179233 \n",
            "\t8001/100000: loss: 0.6254018 \n",
            "\t9001/100000: loss: 0.5399483 \n",
            "\t10001/100000: loss: 0.4865454 \n",
            "\t11001/100000: loss: 0.4586742 \n",
            "\t12001/100000: loss: 0.4483366 \n",
            "\t13001/100000: loss: 0.4436103 \n",
            "\t14001/100000: loss: 0.4298256 \n",
            "\t15001/100000: loss: 0.4180274 \n",
            "\t16001/100000: loss: 0.4078645 \n",
            "\t17001/100000: loss: 0.3987384 \n",
            "\t18001/100000: loss: 0.3903132 \n",
            "\t19001/100000: loss: 0.3822236 \n",
            "\t20001/100000: loss: 0.3746312 \n",
            "\t21001/100000: loss: 0.3678826 \n",
            "\t22001/100000: loss: 0.3617265 \n",
            "\t23001/100000: loss: 0.3560369 \n",
            "\t24001/100000: loss: 0.3507447 \n",
            "\t25001/100000: loss: 0.3458109 \n",
            "\t26001/100000: loss: 0.3412099 \n",
            "\t27001/100000: loss: 0.3369379 \n",
            "\t28001/100000: loss: 0.3330427 \n",
            "\t29001/100000: loss: 0.3295566 \n",
            "\t30001/100000: loss: 0.3260975 \n",
            "\t31001/100000: loss: 0.3223508 \n",
            "\t32001/100000: loss: 0.3189938 \n",
            "\t33001/100000: loss: 0.3158764 \n",
            "\t34001/100000: loss: 0.3129532 \n",
            "\t35001/100000: loss: 0.3102029 \n",
            "\t36001/100000: loss: 0.3076043 \n",
            "\t37001/100000: loss: 0.3051496 \n",
            "\t38001/100000: loss: 0.3028212 \n",
            "\t39001/100000: loss: 0.3006147 \n",
            "\t40001/100000: loss: 0.2985158 \n",
            "\t41001/100000: loss: 0.2965199 \n",
            "\t42001/100000: loss: 0.2946153 \n",
            "\t43001/100000: loss: 0.2927985 \n",
            "\t44001/100000: loss: 0.2910630 \n",
            "\t45001/100000: loss: 0.2894009 \n",
            "\t46001/100000: loss: 0.2878097 \n",
            "\t47001/100000: loss: 0.2862828 \n",
            "\t48001/100000: loss: 0.2848163 \n",
            "\t49001/100000: loss: 0.2834053 \n",
            "\t50001/100000: loss: 0.2820475 \n",
            "\t51001/100000: loss: 0.2807367 \n",
            "\t52001/100000: loss: 0.2794723 \n",
            "\t53001/100000: loss: 0.2782514 \n",
            "\t54001/100000: loss: 0.2770704 \n",
            "\t55001/100000: loss: 0.2759262 \n",
            "\t56001/100000: loss: 0.2748173 \n",
            "\t57001/100000: loss: 0.2737413 \n",
            "\t58001/100000: loss: 0.2726960 \n",
            "\t59001/100000: loss: 0.2716810 \n",
            "\t60001/100000: loss: 0.2706926 \n",
            "\t61001/100000: loss: 0.2697296 \n",
            "\t62001/100000: loss: 0.2687919 \n",
            "\t63001/100000: loss: 0.2678759 \n",
            "\t64001/100000: loss: 0.2669829 \n",
            "\t65001/100000: loss: 0.2661107 \n",
            "\t66001/100000: loss: 0.2652594 \n",
            "\t67001/100000: loss: 0.2644249 \n",
            "\t68001/100000: loss: 0.2636076 \n",
            "\t69001/100000: loss: 0.2628087 \n",
            "\t70001/100000: loss: 0.2620271 \n",
            "\t71001/100000: loss: 0.2612577 \n",
            "\t72001/100000: loss: 0.2605046 \n",
            "\t73001/100000: loss: 0.2597640 \n",
            "\t74001/100000: loss: 0.2590380 \n",
            "\t75001/100000: loss: 0.2583239 \n",
            "\t76001/100000: loss: 0.2576211 \n",
            "\t77001/100000: loss: 0.2569309 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\t78001/100000: loss: 0.2562526 \n",
            "\t79001/100000: loss: 0.2555827 \n",
            "\t80001/100000: loss: 0.2549240 \n",
            "\t81001/100000: loss: 0.2542750 \n",
            "\t82001/100000: loss: 0.2536360 \n",
            "\t83001/100000: loss: 0.2530054 \n",
            "\t84001/100000: loss: 0.2523832 \n",
            "\t85001/100000: loss: 0.2517692 \n",
            "\t86001/100000: loss: 0.2511643 \n",
            "\t87001/100000: loss: 0.2505638 \n",
            "\t88001/100000: loss: 0.2499743 \n",
            "\t89001/100000: loss: 0.2493915 \n",
            "\t90001/100000: loss: 0.2488150 \n",
            "\t91001/100000: loss: 0.2482455 \n",
            "\t92001/100000: loss: 0.2476819 \n",
            "\t93001/100000: loss: 0.2471237 \n",
            "\t94001/100000: loss: 0.2465719 \n",
            "\t95001/100000: loss: 0.2460266 \n",
            "\t96001/100000: loss: 0.2454842 \n",
            "\t97001/100000: loss: 0.2449513 \n",
            "\t98001/100000: loss: 0.2444225 \n",
            "\t99001/100000: loss: 0.2439007 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rH383vxBkXFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "de78837e-c44a-468d-e353-a696466c49e4"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe8ef9fc7b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe94b4177f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7cUh8xzhmR8f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, data, target):\n",
        "  with torch.no_grad():\n",
        "    out = model(data)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uTmTzgxdmjvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3df91b45-a451-47ed-f3a7-eeee36181734"
      },
      "cell_type": "code",
      "source": [
        "res = []\n",
        "mse = test(net, X_t, y_t)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ySgLddFr99b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69a652a5-2719-4b79-fca0-15c33ca50019"
      },
      "cell_type": "code",
      "source": [
        "print(X_t[1], y_t[1], mse[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([8.5945, 9.5916]) tensor([8.5945]) tensor([8.7337])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-qcWDDry5RTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNIST recognition"
      ]
    },
    {
      "metadata": {
        "id": "w-gyieNRcJNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rv-Nic2ECfMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6309ae31-4fbb-409d-f9a2-38db68d3e6c8"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='../../data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SrKIsw8lDIBE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recurrent neural network (many-to-one)\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes, linear=True):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    if linear:\n",
        "      self.out = nn.Linear(hidden_size, num_classes)\n",
        "    else:\n",
        "      self.out = NALU(128, 100, num_classes, 2)\n",
        "    \n",
        "  def forward(self, input): \n",
        "    # Set initial hidden and cell states \n",
        "    hidden = torch.zeros(self.num_layers, input.size(0), self.hidden_size).to(device) \n",
        "    cell_state = torch.zeros(self.num_layers, input.size(0), self.hidden_size).to(device)\n",
        "    # Forward propagate LSTM\n",
        "    output, _ = self.lstm(input, (hidden, cell_state))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "    # Decode the hidden state of the last time step\n",
        "    output = self.out(output[:, -1, :])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3FHjQqZGEJu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = RNN(input_size, hidden_size, num_layers, num_classes, False).to(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZqOYItM9GwV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train(model, optimizer, train_loader, criterion, num_epochs):\n",
        "  all_losses = []\n",
        "  total_step = len(train_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "      labels = labels.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      all_losses.append(loss)\n",
        "      \n",
        "      # Backward and optimize\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "        \n",
        "      if (i+1) % 100 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "  return all_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rz2v5X68H4ap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 30617
        },
        "outputId": "4068a297-aa10-4a99-cfac-ecd8c0146abd"
      },
      "cell_type": "code",
      "source": [
        "all_losses = train(model, optimizer, train_loader, criterion, num_epochs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [100/1875], Loss: 9.9565\n",
            "Epoch [1/100], Step [200/1875], Loss: 3.7254\n",
            "Epoch [1/100], Step [300/1875], Loss: 3.0221\n",
            "Epoch [1/100], Step [400/1875], Loss: 3.2592\n",
            "Epoch [1/100], Step [500/1875], Loss: 2.0817\n",
            "Epoch [1/100], Step [600/1875], Loss: 2.2614\n",
            "Epoch [1/100], Step [700/1875], Loss: 2.3010\n",
            "Epoch [1/100], Step [800/1875], Loss: 2.3388\n",
            "Epoch [1/100], Step [900/1875], Loss: 2.0606\n",
            "Epoch [1/100], Step [1000/1875], Loss: 2.2694\n",
            "Epoch [1/100], Step [1100/1875], Loss: 2.1438\n",
            "Epoch [1/100], Step [1200/1875], Loss: 2.7314\n",
            "Epoch [1/100], Step [1300/1875], Loss: 2.0363\n",
            "Epoch [1/100], Step [1400/1875], Loss: 2.0855\n",
            "Epoch [1/100], Step [1500/1875], Loss: 2.0446\n",
            "Epoch [1/100], Step [1600/1875], Loss: 2.1368\n",
            "Epoch [1/100], Step [1700/1875], Loss: 2.1489\n",
            "Epoch [1/100], Step [1800/1875], Loss: 2.1537\n",
            "Epoch [2/100], Step [100/1875], Loss: 1.8390\n",
            "Epoch [2/100], Step [200/1875], Loss: 2.1190\n",
            "Epoch [2/100], Step [300/1875], Loss: 2.0269\n",
            "Epoch [2/100], Step [400/1875], Loss: 2.1444\n",
            "Epoch [2/100], Step [500/1875], Loss: 2.0670\n",
            "Epoch [2/100], Step [600/1875], Loss: 1.7351\n",
            "Epoch [2/100], Step [700/1875], Loss: 1.9487\n",
            "Epoch [2/100], Step [800/1875], Loss: 1.8185\n",
            "Epoch [2/100], Step [900/1875], Loss: 1.9734\n",
            "Epoch [2/100], Step [1000/1875], Loss: 1.8184\n",
            "Epoch [2/100], Step [1100/1875], Loss: 1.7350\n",
            "Epoch [2/100], Step [1200/1875], Loss: 2.1204\n",
            "Epoch [2/100], Step [1300/1875], Loss: 1.7919\n",
            "Epoch [2/100], Step [1400/1875], Loss: 2.0317\n",
            "Epoch [2/100], Step [1500/1875], Loss: 1.6459\n",
            "Epoch [2/100], Step [1600/1875], Loss: 2.0445\n",
            "Epoch [2/100], Step [1700/1875], Loss: 1.6488\n",
            "Epoch [2/100], Step [1800/1875], Loss: 2.9035\n",
            "Epoch [3/100], Step [100/1875], Loss: 2.5302\n",
            "Epoch [3/100], Step [200/1875], Loss: 2.0580\n",
            "Epoch [3/100], Step [300/1875], Loss: 1.9665\n",
            "Epoch [3/100], Step [400/1875], Loss: 4.7211\n",
            "Epoch [3/100], Step [500/1875], Loss: 2.3426\n",
            "Epoch [3/100], Step [600/1875], Loss: 2.1981\n",
            "Epoch [3/100], Step [700/1875], Loss: 2.0872\n",
            "Epoch [3/100], Step [800/1875], Loss: 2.2098\n",
            "Epoch [3/100], Step [900/1875], Loss: 2.2854\n",
            "Epoch [3/100], Step [1000/1875], Loss: 2.0433\n",
            "Epoch [3/100], Step [1100/1875], Loss: 2.2464\n",
            "Epoch [3/100], Step [1200/1875], Loss: 2.0972\n",
            "Epoch [3/100], Step [1300/1875], Loss: 1.9346\n",
            "Epoch [3/100], Step [1400/1875], Loss: 2.1706\n",
            "Epoch [3/100], Step [1500/1875], Loss: 1.9972\n",
            "Epoch [3/100], Step [1600/1875], Loss: 1.8293\n",
            "Epoch [3/100], Step [1700/1875], Loss: 1.9931\n",
            "Epoch [3/100], Step [1800/1875], Loss: 1.9635\n",
            "Epoch [4/100], Step [100/1875], Loss: 2.0162\n",
            "Epoch [4/100], Step [200/1875], Loss: 2.0547\n",
            "Epoch [4/100], Step [300/1875], Loss: 1.9160\n",
            "Epoch [4/100], Step [400/1875], Loss: 1.9942\n",
            "Epoch [4/100], Step [500/1875], Loss: 2.0430\n",
            "Epoch [4/100], Step [600/1875], Loss: 2.0710\n",
            "Epoch [4/100], Step [700/1875], Loss: 1.9757\n",
            "Epoch [4/100], Step [800/1875], Loss: 1.8703\n",
            "Epoch [4/100], Step [900/1875], Loss: 1.7218\n",
            "Epoch [4/100], Step [1000/1875], Loss: 2.0644\n",
            "Epoch [4/100], Step [1100/1875], Loss: 1.8738\n",
            "Epoch [4/100], Step [1200/1875], Loss: 1.7677\n",
            "Epoch [4/100], Step [1300/1875], Loss: 1.8941\n",
            "Epoch [4/100], Step [1400/1875], Loss: 1.8007\n",
            "Epoch [4/100], Step [1500/1875], Loss: 2.2151\n",
            "Epoch [4/100], Step [1600/1875], Loss: 2.2448\n",
            "Epoch [4/100], Step [1700/1875], Loss: 2.2982\n",
            "Epoch [4/100], Step [1800/1875], Loss: 2.3824\n",
            "Epoch [5/100], Step [100/1875], Loss: 2.2830\n",
            "Epoch [5/100], Step [200/1875], Loss: 2.2591\n",
            "Epoch [5/100], Step [300/1875], Loss: 2.1914\n",
            "Epoch [5/100], Step [400/1875], Loss: 2.2018\n",
            "Epoch [5/100], Step [500/1875], Loss: 2.1347\n",
            "Epoch [5/100], Step [600/1875], Loss: 2.1503\n",
            "Epoch [5/100], Step [700/1875], Loss: 2.3269\n",
            "Epoch [5/100], Step [800/1875], Loss: 2.3199\n",
            "Epoch [5/100], Step [900/1875], Loss: 2.3287\n",
            "Epoch [5/100], Step [1000/1875], Loss: 2.2618\n",
            "Epoch [5/100], Step [1100/1875], Loss: 2.2738\n",
            "Epoch [5/100], Step [1200/1875], Loss: 2.3208\n",
            "Epoch [5/100], Step [1300/1875], Loss: 2.2633\n",
            "Epoch [5/100], Step [1400/1875], Loss: 2.1236\n",
            "Epoch [5/100], Step [1500/1875], Loss: 2.1908\n",
            "Epoch [5/100], Step [1600/1875], Loss: 2.1419\n",
            "Epoch [5/100], Step [1700/1875], Loss: 2.2245\n",
            "Epoch [5/100], Step [1800/1875], Loss: 2.1059\n",
            "Epoch [6/100], Step [100/1875], Loss: 2.0785\n",
            "Epoch [6/100], Step [200/1875], Loss: 2.3232\n",
            "Epoch [6/100], Step [300/1875], Loss: 2.1628\n",
            "Epoch [6/100], Step [400/1875], Loss: 2.2259\n",
            "Epoch [6/100], Step [500/1875], Loss: 2.1700\n",
            "Epoch [6/100], Step [600/1875], Loss: 2.3159\n",
            "Epoch [6/100], Step [700/1875], Loss: 1.9790\n",
            "Epoch [6/100], Step [800/1875], Loss: 2.3618\n",
            "Epoch [6/100], Step [900/1875], Loss: 1.6935\n",
            "Epoch [6/100], Step [1000/1875], Loss: 1.8606\n",
            "Epoch [6/100], Step [1100/1875], Loss: 3.8931\n",
            "Epoch [6/100], Step [1200/1875], Loss: 2.3137\n",
            "Epoch [6/100], Step [1300/1875], Loss: 2.2958\n",
            "Epoch [6/100], Step [1400/1875], Loss: 2.2726\n",
            "Epoch [6/100], Step [1500/1875], Loss: 2.2642\n",
            "Epoch [6/100], Step [1600/1875], Loss: 1.9118\n",
            "Epoch [6/100], Step [1700/1875], Loss: 2.1387\n",
            "Epoch [6/100], Step [1800/1875], Loss: 2.2399\n",
            "Epoch [7/100], Step [100/1875], Loss: 2.1572\n",
            "Epoch [7/100], Step [200/1875], Loss: 2.3175\n",
            "Epoch [7/100], Step [300/1875], Loss: 2.2610\n",
            "Epoch [7/100], Step [400/1875], Loss: 2.2766\n",
            "Epoch [7/100], Step [500/1875], Loss: 2.2797\n",
            "Epoch [7/100], Step [600/1875], Loss: 2.2738\n",
            "Epoch [7/100], Step [700/1875], Loss: 2.2702\n",
            "Epoch [7/100], Step [800/1875], Loss: 2.1664\n",
            "Epoch [7/100], Step [900/1875], Loss: 2.0813\n",
            "Epoch [7/100], Step [1000/1875], Loss: 2.0080\n",
            "Epoch [7/100], Step [1100/1875], Loss: 1.8789\n",
            "Epoch [7/100], Step [1200/1875], Loss: 1.9648\n",
            "Epoch [7/100], Step [1300/1875], Loss: 1.7776\n",
            "Epoch [7/100], Step [1400/1875], Loss: 1.6261\n",
            "Epoch [7/100], Step [1500/1875], Loss: 2.3077\n",
            "Epoch [7/100], Step [1600/1875], Loss: 2.2710\n",
            "Epoch [7/100], Step [1700/1875], Loss: 2.2400\n",
            "Epoch [7/100], Step [1800/1875], Loss: 2.2903\n",
            "Epoch [8/100], Step [100/1875], Loss: 2.2891\n",
            "Epoch [8/100], Step [200/1875], Loss: 2.2947\n",
            "Epoch [8/100], Step [300/1875], Loss: 2.1309\n",
            "Epoch [8/100], Step [400/1875], Loss: 2.2306\n",
            "Epoch [8/100], Step [500/1875], Loss: 2.3716\n",
            "Epoch [8/100], Step [600/1875], Loss: 1.8282\n",
            "Epoch [8/100], Step [700/1875], Loss: 1.6995\n",
            "Epoch [8/100], Step [800/1875], Loss: 1.9717\n",
            "Epoch [8/100], Step [900/1875], Loss: 2.0314\n",
            "Epoch [8/100], Step [1000/1875], Loss: 1.2667\n",
            "Epoch [8/100], Step [1100/1875], Loss: 1.5508\n",
            "Epoch [8/100], Step [1200/1875], Loss: 1.0349\n",
            "Epoch [8/100], Step [1300/1875], Loss: 1.4612\n",
            "Epoch [8/100], Step [1400/1875], Loss: 1.3638\n",
            "Epoch [8/100], Step [1500/1875], Loss: 1.0109\n",
            "Epoch [8/100], Step [1600/1875], Loss: 0.7109\n",
            "Epoch [8/100], Step [1700/1875], Loss: 1.0725\n",
            "Epoch [8/100], Step [1800/1875], Loss: 0.5290\n",
            "Epoch [9/100], Step [100/1875], Loss: 0.5511\n",
            "Epoch [9/100], Step [200/1875], Loss: 0.9302\n",
            "Epoch [9/100], Step [300/1875], Loss: 0.3394\n",
            "Epoch [9/100], Step [400/1875], Loss: 0.6847\n",
            "Epoch [9/100], Step [500/1875], Loss: 0.5557\n",
            "Epoch [9/100], Step [600/1875], Loss: 0.2286\n",
            "Epoch [9/100], Step [700/1875], Loss: 0.5676\n",
            "Epoch [9/100], Step [800/1875], Loss: 0.5367\n",
            "Epoch [9/100], Step [900/1875], Loss: 0.5777\n",
            "Epoch [9/100], Step [1000/1875], Loss: 0.4494\n",
            "Epoch [9/100], Step [1100/1875], Loss: 0.6519\n",
            "Epoch [9/100], Step [1200/1875], Loss: 0.2488\n",
            "Epoch [9/100], Step [1300/1875], Loss: 0.4514\n",
            "Epoch [9/100], Step [1400/1875], Loss: 0.2957\n",
            "Epoch [9/100], Step [1500/1875], Loss: 0.2543\n",
            "Epoch [9/100], Step [1600/1875], Loss: 0.2697\n",
            "Epoch [9/100], Step [1700/1875], Loss: 0.5866\n",
            "Epoch [9/100], Step [1800/1875], Loss: 0.1504\n",
            "Epoch [10/100], Step [100/1875], Loss: 0.0764\n",
            "Epoch [10/100], Step [200/1875], Loss: 0.4332\n",
            "Epoch [10/100], Step [300/1875], Loss: 0.1506\n",
            "Epoch [10/100], Step [400/1875], Loss: 0.4480\n",
            "Epoch [10/100], Step [500/1875], Loss: 0.2333\n",
            "Epoch [10/100], Step [600/1875], Loss: 0.0440\n",
            "Epoch [10/100], Step [700/1875], Loss: 0.8270\n",
            "Epoch [10/100], Step [800/1875], Loss: 0.1883\n",
            "Epoch [10/100], Step [900/1875], Loss: 0.3419\n",
            "Epoch [10/100], Step [1000/1875], Loss: 0.7255\n",
            "Epoch [10/100], Step [1100/1875], Loss: 0.3178\n",
            "Epoch [10/100], Step [1200/1875], Loss: 0.6516\n",
            "Epoch [10/100], Step [1300/1875], Loss: 0.0755\n",
            "Epoch [10/100], Step [1400/1875], Loss: 0.2108\n",
            "Epoch [10/100], Step [1500/1875], Loss: 0.1068\n",
            "Epoch [10/100], Step [1600/1875], Loss: 0.1417\n",
            "Epoch [10/100], Step [1700/1875], Loss: 0.1956\n",
            "Epoch [10/100], Step [1800/1875], Loss: 0.0884\n",
            "Epoch [11/100], Step [100/1875], Loss: 0.0470\n",
            "Epoch [11/100], Step [200/1875], Loss: 0.0882\n",
            "Epoch [11/100], Step [300/1875], Loss: 0.1621\n",
            "Epoch [11/100], Step [400/1875], Loss: 0.2158\n",
            "Epoch [11/100], Step [500/1875], Loss: 0.1224\n",
            "Epoch [11/100], Step [600/1875], Loss: 0.1396\n",
            "Epoch [11/100], Step [700/1875], Loss: 0.0369\n",
            "Epoch [11/100], Step [800/1875], Loss: 0.3014\n",
            "Epoch [11/100], Step [900/1875], Loss: 0.1138\n",
            "Epoch [11/100], Step [1000/1875], Loss: 0.2930\n",
            "Epoch [11/100], Step [1100/1875], Loss: 0.0124\n",
            "Epoch [11/100], Step [1200/1875], Loss: 0.6429\n",
            "Epoch [11/100], Step [1300/1875], Loss: 0.0296\n",
            "Epoch [11/100], Step [1400/1875], Loss: 0.0669\n",
            "Epoch [11/100], Step [1500/1875], Loss: 0.2709\n",
            "Epoch [11/100], Step [1600/1875], Loss: 0.2201\n",
            "Epoch [11/100], Step [1700/1875], Loss: 0.0430\n",
            "Epoch [11/100], Step [1800/1875], Loss: 0.2405\n",
            "Epoch [12/100], Step [100/1875], Loss: 0.1429\n",
            "Epoch [12/100], Step [200/1875], Loss: 0.0333\n",
            "Epoch [12/100], Step [300/1875], Loss: 0.0342\n",
            "Epoch [12/100], Step [400/1875], Loss: 0.1163\n",
            "Epoch [12/100], Step [500/1875], Loss: 0.0874\n",
            "Epoch [12/100], Step [600/1875], Loss: 0.0100\n",
            "Epoch [12/100], Step [700/1875], Loss: 0.1086\n",
            "Epoch [12/100], Step [800/1875], Loss: 0.1240\n",
            "Epoch [12/100], Step [900/1875], Loss: 0.0366\n",
            "Epoch [12/100], Step [1000/1875], Loss: 0.1523\n",
            "Epoch [12/100], Step [1100/1875], Loss: 0.0094\n",
            "Epoch [12/100], Step [1200/1875], Loss: 0.0053\n",
            "Epoch [12/100], Step [1300/1875], Loss: 0.0581\n",
            "Epoch [12/100], Step [1400/1875], Loss: 1.2839\n",
            "Epoch [12/100], Step [1500/1875], Loss: 0.1604\n",
            "Epoch [12/100], Step [1600/1875], Loss: 0.2145\n",
            "Epoch [12/100], Step [1700/1875], Loss: 0.0452\n",
            "Epoch [12/100], Step [1800/1875], Loss: 0.0483\n",
            "Epoch [13/100], Step [100/1875], Loss: 0.2045\n",
            "Epoch [13/100], Step [200/1875], Loss: 0.0787\n",
            "Epoch [13/100], Step [300/1875], Loss: 0.0024\n",
            "Epoch [13/100], Step [400/1875], Loss: 0.1202\n",
            "Epoch [13/100], Step [500/1875], Loss: 0.0954\n",
            "Epoch [13/100], Step [600/1875], Loss: 0.1735\n",
            "Epoch [13/100], Step [700/1875], Loss: 0.0073\n",
            "Epoch [13/100], Step [800/1875], Loss: 0.0180\n",
            "Epoch [13/100], Step [900/1875], Loss: 0.0265\n",
            "Epoch [13/100], Step [1000/1875], Loss: 0.1821\n",
            "Epoch [13/100], Step [1100/1875], Loss: 0.1483\n",
            "Epoch [13/100], Step [1200/1875], Loss: 0.1008\n",
            "Epoch [13/100], Step [1300/1875], Loss: 0.1601\n",
            "Epoch [13/100], Step [1400/1875], Loss: 0.0667\n",
            "Epoch [13/100], Step [1500/1875], Loss: 0.0606\n",
            "Epoch [13/100], Step [1600/1875], Loss: 0.1127\n",
            "Epoch [13/100], Step [1700/1875], Loss: 0.0626\n",
            "Epoch [13/100], Step [1800/1875], Loss: 0.0945\n",
            "Epoch [14/100], Step [100/1875], Loss: 0.0547\n",
            "Epoch [14/100], Step [200/1875], Loss: 0.1516\n",
            "Epoch [14/100], Step [300/1875], Loss: 0.0609\n",
            "Epoch [14/100], Step [400/1875], Loss: 0.1180\n",
            "Epoch [14/100], Step [500/1875], Loss: 0.0010\n",
            "Epoch [14/100], Step [600/1875], Loss: 0.2088\n",
            "Epoch [14/100], Step [700/1875], Loss: 0.0182\n",
            "Epoch [14/100], Step [800/1875], Loss: 0.2988\n",
            "Epoch [14/100], Step [900/1875], Loss: 0.0192\n",
            "Epoch [14/100], Step [1000/1875], Loss: 0.1548\n",
            "Epoch [14/100], Step [1100/1875], Loss: 0.0228\n",
            "Epoch [14/100], Step [1200/1875], Loss: 0.1109\n",
            "Epoch [14/100], Step [1300/1875], Loss: 0.1247\n",
            "Epoch [14/100], Step [1400/1875], Loss: 0.2343\n",
            "Epoch [14/100], Step [1500/1875], Loss: 0.0344\n",
            "Epoch [14/100], Step [1600/1875], Loss: 0.0595\n",
            "Epoch [14/100], Step [1700/1875], Loss: 0.0753\n",
            "Epoch [14/100], Step [1800/1875], Loss: 0.0045\n",
            "Epoch [15/100], Step [100/1875], Loss: 0.0443\n",
            "Epoch [15/100], Step [200/1875], Loss: 0.0016\n",
            "Epoch [15/100], Step [300/1875], Loss: 0.0042\n",
            "Epoch [15/100], Step [400/1875], Loss: 0.1457\n",
            "Epoch [15/100], Step [500/1875], Loss: 0.0091\n",
            "Epoch [15/100], Step [600/1875], Loss: 0.1537\n",
            "Epoch [15/100], Step [700/1875], Loss: 0.0299\n",
            "Epoch [15/100], Step [800/1875], Loss: 0.0005\n",
            "Epoch [15/100], Step [900/1875], Loss: 0.0933\n",
            "Epoch [15/100], Step [1000/1875], Loss: 0.1399\n",
            "Epoch [15/100], Step [1100/1875], Loss: 0.0105\n",
            "Epoch [15/100], Step [1200/1875], Loss: 0.0665\n",
            "Epoch [15/100], Step [1300/1875], Loss: 0.0925\n",
            "Epoch [15/100], Step [1400/1875], Loss: 0.3291\n",
            "Epoch [15/100], Step [1500/1875], Loss: 0.0560\n",
            "Epoch [15/100], Step [1600/1875], Loss: 0.2328\n",
            "Epoch [15/100], Step [1700/1875], Loss: 0.0013\n",
            "Epoch [15/100], Step [1800/1875], Loss: 0.0095\n",
            "Epoch [16/100], Step [100/1875], Loss: 0.0252\n",
            "Epoch [16/100], Step [200/1875], Loss: 0.1317\n",
            "Epoch [16/100], Step [300/1875], Loss: 0.0243\n",
            "Epoch [16/100], Step [400/1875], Loss: 0.0103\n",
            "Epoch [16/100], Step [500/1875], Loss: 0.0009\n",
            "Epoch [16/100], Step [600/1875], Loss: 0.0359\n",
            "Epoch [16/100], Step [700/1875], Loss: 0.3582\n",
            "Epoch [16/100], Step [800/1875], Loss: 0.0486\n",
            "Epoch [16/100], Step [900/1875], Loss: 0.0077\n",
            "Epoch [16/100], Step [1000/1875], Loss: 0.1811\n",
            "Epoch [16/100], Step [1100/1875], Loss: 0.0087\n",
            "Epoch [16/100], Step [1200/1875], Loss: 0.0522\n",
            "Epoch [16/100], Step [1300/1875], Loss: 0.0542\n",
            "Epoch [16/100], Step [1400/1875], Loss: 0.1428\n",
            "Epoch [16/100], Step [1500/1875], Loss: 0.0561\n",
            "Epoch [16/100], Step [1600/1875], Loss: 0.0344\n",
            "Epoch [16/100], Step [1700/1875], Loss: 0.6031\n",
            "Epoch [16/100], Step [1800/1875], Loss: 0.3135\n",
            "Epoch [17/100], Step [100/1875], Loss: 0.0454\n",
            "Epoch [17/100], Step [200/1875], Loss: 0.0003\n",
            "Epoch [17/100], Step [300/1875], Loss: 0.1401\n",
            "Epoch [17/100], Step [400/1875], Loss: 0.0425\n",
            "Epoch [17/100], Step [500/1875], Loss: 0.1082\n",
            "Epoch [17/100], Step [600/1875], Loss: 0.0163\n",
            "Epoch [17/100], Step [700/1875], Loss: 0.0160\n",
            "Epoch [17/100], Step [800/1875], Loss: 0.1067\n",
            "Epoch [17/100], Step [900/1875], Loss: 0.0885\n",
            "Epoch [17/100], Step [1000/1875], Loss: 0.0085\n",
            "Epoch [17/100], Step [1100/1875], Loss: 0.0221\n",
            "Epoch [17/100], Step [1200/1875], Loss: 0.0298\n",
            "Epoch [17/100], Step [1300/1875], Loss: 0.0047\n",
            "Epoch [17/100], Step [1400/1875], Loss: 0.1290\n",
            "Epoch [17/100], Step [1500/1875], Loss: 0.0058\n",
            "Epoch [17/100], Step [1600/1875], Loss: 0.0213\n",
            "Epoch [17/100], Step [1700/1875], Loss: 0.0173\n",
            "Epoch [17/100], Step [1800/1875], Loss: 0.0019\n",
            "Epoch [18/100], Step [100/1875], Loss: 0.0448\n",
            "Epoch [18/100], Step [200/1875], Loss: 0.0763\n",
            "Epoch [18/100], Step [300/1875], Loss: 0.0322\n",
            "Epoch [18/100], Step [400/1875], Loss: 0.0009\n",
            "Epoch [18/100], Step [500/1875], Loss: 0.0783\n",
            "Epoch [18/100], Step [600/1875], Loss: 0.0299\n",
            "Epoch [18/100], Step [700/1875], Loss: 0.0470\n",
            "Epoch [18/100], Step [800/1875], Loss: 0.0005\n",
            "Epoch [18/100], Step [900/1875], Loss: 0.0207\n",
            "Epoch [18/100], Step [1000/1875], Loss: 0.0767\n",
            "Epoch [18/100], Step [1100/1875], Loss: 0.0263\n",
            "Epoch [18/100], Step [1200/1875], Loss: 0.0082\n",
            "Epoch [18/100], Step [1300/1875], Loss: 0.0064\n",
            "Epoch [18/100], Step [1400/1875], Loss: 0.0020\n",
            "Epoch [18/100], Step [1500/1875], Loss: 0.0058\n",
            "Epoch [18/100], Step [1600/1875], Loss: 0.0392\n",
            "Epoch [18/100], Step [1700/1875], Loss: 0.0341\n",
            "Epoch [18/100], Step [1800/1875], Loss: 0.0175\n",
            "Epoch [19/100], Step [100/1875], Loss: 0.0538\n",
            "Epoch [19/100], Step [200/1875], Loss: 0.0018\n",
            "Epoch [19/100], Step [300/1875], Loss: 0.0392\n",
            "Epoch [19/100], Step [400/1875], Loss: 0.0206\n",
            "Epoch [19/100], Step [500/1875], Loss: 0.0287\n",
            "Epoch [19/100], Step [600/1875], Loss: 0.0214\n",
            "Epoch [19/100], Step [700/1875], Loss: 0.0029\n",
            "Epoch [19/100], Step [800/1875], Loss: 0.0168\n",
            "Epoch [19/100], Step [900/1875], Loss: 0.0449\n",
            "Epoch [19/100], Step [1000/1875], Loss: 0.1251\n",
            "Epoch [19/100], Step [1100/1875], Loss: 0.0281\n",
            "Epoch [19/100], Step [1200/1875], Loss: 0.0009\n",
            "Epoch [19/100], Step [1300/1875], Loss: 0.0006\n",
            "Epoch [19/100], Step [1400/1875], Loss: 0.0472\n",
            "Epoch [19/100], Step [1500/1875], Loss: 0.0138\n",
            "Epoch [19/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [19/100], Step [1700/1875], Loss: 0.0070\n",
            "Epoch [19/100], Step [1800/1875], Loss: 0.0418\n",
            "Epoch [20/100], Step [100/1875], Loss: 0.0359\n",
            "Epoch [20/100], Step [200/1875], Loss: 0.0388\n",
            "Epoch [20/100], Step [300/1875], Loss: 0.0276\n",
            "Epoch [20/100], Step [400/1875], Loss: 0.0659\n",
            "Epoch [20/100], Step [500/1875], Loss: 0.0094\n",
            "Epoch [20/100], Step [600/1875], Loss: 0.0170\n",
            "Epoch [20/100], Step [700/1875], Loss: 0.0529\n",
            "Epoch [20/100], Step [800/1875], Loss: 0.0029\n",
            "Epoch [20/100], Step [900/1875], Loss: 0.0145\n",
            "Epoch [20/100], Step [1000/1875], Loss: 0.0415\n",
            "Epoch [20/100], Step [1100/1875], Loss: 0.0243\n",
            "Epoch [20/100], Step [1200/1875], Loss: 0.0662\n",
            "Epoch [20/100], Step [1300/1875], Loss: 0.0036\n",
            "Epoch [20/100], Step [1400/1875], Loss: 0.0729\n",
            "Epoch [20/100], Step [1500/1875], Loss: 0.0012\n",
            "Epoch [20/100], Step [1600/1875], Loss: 0.0188\n",
            "Epoch [20/100], Step [1700/1875], Loss: 0.7150\n",
            "Epoch [20/100], Step [1800/1875], Loss: 0.2296\n",
            "Epoch [21/100], Step [100/1875], Loss: 0.1392\n",
            "Epoch [21/100], Step [200/1875], Loss: 0.0114\n",
            "Epoch [21/100], Step [300/1875], Loss: 0.0091\n",
            "Epoch [21/100], Step [400/1875], Loss: 0.0089\n",
            "Epoch [21/100], Step [500/1875], Loss: 0.0043\n",
            "Epoch [21/100], Step [600/1875], Loss: 0.0877\n",
            "Epoch [21/100], Step [700/1875], Loss: 0.0095\n",
            "Epoch [21/100], Step [800/1875], Loss: 0.0908\n",
            "Epoch [21/100], Step [900/1875], Loss: 0.0211\n",
            "Epoch [21/100], Step [1000/1875], Loss: 0.0238\n",
            "Epoch [21/100], Step [1100/1875], Loss: 0.1466\n",
            "Epoch [21/100], Step [1200/1875], Loss: 0.0626\n",
            "Epoch [21/100], Step [1300/1875], Loss: 0.0037\n",
            "Epoch [21/100], Step [1400/1875], Loss: 0.0087\n",
            "Epoch [21/100], Step [1500/1875], Loss: 0.0059\n",
            "Epoch [21/100], Step [1600/1875], Loss: 0.1395\n",
            "Epoch [21/100], Step [1700/1875], Loss: 0.0067\n",
            "Epoch [21/100], Step [1800/1875], Loss: 0.0010\n",
            "Epoch [22/100], Step [100/1875], Loss: 0.1267\n",
            "Epoch [22/100], Step [200/1875], Loss: 0.0008\n",
            "Epoch [22/100], Step [300/1875], Loss: 0.0056\n",
            "Epoch [22/100], Step [400/1875], Loss: 0.0116\n",
            "Epoch [22/100], Step [500/1875], Loss: 0.0208\n",
            "Epoch [22/100], Step [600/1875], Loss: 0.0944\n",
            "Epoch [22/100], Step [700/1875], Loss: 0.0100\n",
            "Epoch [22/100], Step [800/1875], Loss: 0.0046\n",
            "Epoch [22/100], Step [900/1875], Loss: 0.0933\n",
            "Epoch [22/100], Step [1000/1875], Loss: 0.0419\n",
            "Epoch [22/100], Step [1100/1875], Loss: 0.0110\n",
            "Epoch [22/100], Step [1200/1875], Loss: 0.0063\n",
            "Epoch [22/100], Step [1300/1875], Loss: 0.0161\n",
            "Epoch [22/100], Step [1400/1875], Loss: 0.0070\n",
            "Epoch [22/100], Step [1500/1875], Loss: 0.0703\n",
            "Epoch [22/100], Step [1600/1875], Loss: 0.0053\n",
            "Epoch [22/100], Step [1700/1875], Loss: 0.0230\n",
            "Epoch [22/100], Step [1800/1875], Loss: 0.0039\n",
            "Epoch [23/100], Step [100/1875], Loss: 0.0038\n",
            "Epoch [23/100], Step [200/1875], Loss: 0.0023\n",
            "Epoch [23/100], Step [300/1875], Loss: 0.0011\n",
            "Epoch [23/100], Step [400/1875], Loss: 0.0069\n",
            "Epoch [23/100], Step [500/1875], Loss: 0.0036\n",
            "Epoch [23/100], Step [600/1875], Loss: 0.0008\n",
            "Epoch [23/100], Step [700/1875], Loss: 0.0082\n",
            "Epoch [23/100], Step [800/1875], Loss: 0.0547\n",
            "Epoch [23/100], Step [900/1875], Loss: 0.0425\n",
            "Epoch [23/100], Step [1000/1875], Loss: 0.0027\n",
            "Epoch [23/100], Step [1100/1875], Loss: 0.0012\n",
            "Epoch [23/100], Step [1200/1875], Loss: 0.0089\n",
            "Epoch [23/100], Step [1300/1875], Loss: 0.0404\n",
            "Epoch [23/100], Step [1400/1875], Loss: 0.0201\n",
            "Epoch [23/100], Step [1500/1875], Loss: 0.0734\n",
            "Epoch [23/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [23/100], Step [1700/1875], Loss: 0.0766\n",
            "Epoch [23/100], Step [1800/1875], Loss: 0.0158\n",
            "Epoch [24/100], Step [100/1875], Loss: 0.0019\n",
            "Epoch [24/100], Step [200/1875], Loss: 0.0245\n",
            "Epoch [24/100], Step [300/1875], Loss: 0.0016\n",
            "Epoch [24/100], Step [400/1875], Loss: 0.0018\n",
            "Epoch [24/100], Step [500/1875], Loss: 0.0194\n",
            "Epoch [24/100], Step [600/1875], Loss: 0.0199\n",
            "Epoch [24/100], Step [700/1875], Loss: 0.0090\n",
            "Epoch [24/100], Step [800/1875], Loss: 0.0009\n",
            "Epoch [24/100], Step [900/1875], Loss: 0.0174\n",
            "Epoch [24/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [24/100], Step [1100/1875], Loss: 0.0360\n",
            "Epoch [24/100], Step [1200/1875], Loss: 0.0007\n",
            "Epoch [24/100], Step [1300/1875], Loss: 0.0037\n",
            "Epoch [24/100], Step [1400/1875], Loss: 0.1659\n",
            "Epoch [24/100], Step [1500/1875], Loss: 0.1096\n",
            "Epoch [24/100], Step [1600/1875], Loss: 0.0221\n",
            "Epoch [24/100], Step [1700/1875], Loss: 0.0028\n",
            "Epoch [24/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [25/100], Step [100/1875], Loss: 0.0338\n",
            "Epoch [25/100], Step [200/1875], Loss: 0.1038\n",
            "Epoch [25/100], Step [300/1875], Loss: 0.0016\n",
            "Epoch [25/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [25/100], Step [500/1875], Loss: 0.0030\n",
            "Epoch [25/100], Step [600/1875], Loss: 0.0059\n",
            "Epoch [25/100], Step [700/1875], Loss: 0.1330\n",
            "Epoch [25/100], Step [800/1875], Loss: 0.0060\n",
            "Epoch [25/100], Step [900/1875], Loss: 0.0017\n",
            "Epoch [25/100], Step [1000/1875], Loss: 0.0030\n",
            "Epoch [25/100], Step [1100/1875], Loss: 0.0090\n",
            "Epoch [25/100], Step [1200/1875], Loss: 0.0019\n",
            "Epoch [25/100], Step [1300/1875], Loss: 0.0031\n",
            "Epoch [25/100], Step [1400/1875], Loss: 0.0125\n",
            "Epoch [25/100], Step [1500/1875], Loss: 0.0062\n",
            "Epoch [25/100], Step [1600/1875], Loss: 0.0020\n",
            "Epoch [25/100], Step [1700/1875], Loss: 0.0004\n",
            "Epoch [25/100], Step [1800/1875], Loss: 0.0449\n",
            "Epoch [26/100], Step [100/1875], Loss: 0.0125\n",
            "Epoch [26/100], Step [200/1875], Loss: 0.1291\n",
            "Epoch [26/100], Step [300/1875], Loss: 0.0025\n",
            "Epoch [26/100], Step [400/1875], Loss: 0.0138\n",
            "Epoch [26/100], Step [500/1875], Loss: 0.0018\n",
            "Epoch [26/100], Step [600/1875], Loss: 0.0004\n",
            "Epoch [26/100], Step [700/1875], Loss: 0.0020\n",
            "Epoch [26/100], Step [800/1875], Loss: 0.0006\n",
            "Epoch [26/100], Step [900/1875], Loss: 0.0159\n",
            "Epoch [26/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [26/100], Step [1100/1875], Loss: 0.0006\n",
            "Epoch [26/100], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [26/100], Step [1300/1875], Loss: 0.0092\n",
            "Epoch [26/100], Step [1400/1875], Loss: 0.1289\n",
            "Epoch [26/100], Step [1500/1875], Loss: 0.0003\n",
            "Epoch [26/100], Step [1600/1875], Loss: 0.0024\n",
            "Epoch [26/100], Step [1700/1875], Loss: 0.0078\n",
            "Epoch [26/100], Step [1800/1875], Loss: 0.0021\n",
            "Epoch [27/100], Step [100/1875], Loss: 0.1202\n",
            "Epoch [27/100], Step [200/1875], Loss: 0.0188\n",
            "Epoch [27/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [27/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [27/100], Step [500/1875], Loss: 0.0214\n",
            "Epoch [27/100], Step [600/1875], Loss: 0.0007\n",
            "Epoch [27/100], Step [700/1875], Loss: 0.1881\n",
            "Epoch [27/100], Step [800/1875], Loss: 0.0016\n",
            "Epoch [27/100], Step [900/1875], Loss: 0.0008\n",
            "Epoch [27/100], Step [1000/1875], Loss: 0.0209\n",
            "Epoch [27/100], Step [1100/1875], Loss: 0.0009\n",
            "Epoch [27/100], Step [1200/1875], Loss: 0.0296\n",
            "Epoch [27/100], Step [1300/1875], Loss: 0.0116\n",
            "Epoch [27/100], Step [1400/1875], Loss: 0.0032\n",
            "Epoch [27/100], Step [1500/1875], Loss: 0.0010\n",
            "Epoch [27/100], Step [1600/1875], Loss: 0.0223\n",
            "Epoch [27/100], Step [1700/1875], Loss: 0.0017\n",
            "Epoch [27/100], Step [1800/1875], Loss: 0.2219\n",
            "Epoch [28/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [28/100], Step [200/1875], Loss: 0.0014\n",
            "Epoch [28/100], Step [300/1875], Loss: 0.0005\n",
            "Epoch [28/100], Step [400/1875], Loss: 0.0004\n",
            "Epoch [28/100], Step [500/1875], Loss: 0.2971\n",
            "Epoch [28/100], Step [600/1875], Loss: 0.0005\n",
            "Epoch [28/100], Step [700/1875], Loss: 0.0006\n",
            "Epoch [28/100], Step [800/1875], Loss: 0.0202\n",
            "Epoch [28/100], Step [900/1875], Loss: 0.0051\n",
            "Epoch [28/100], Step [1000/1875], Loss: 0.0256\n",
            "Epoch [28/100], Step [1100/1875], Loss: 0.0097\n",
            "Epoch [28/100], Step [1200/1875], Loss: 0.0920\n",
            "Epoch [28/100], Step [1300/1875], Loss: 0.0022\n",
            "Epoch [28/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [28/100], Step [1500/1875], Loss: 0.0048\n",
            "Epoch [28/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [28/100], Step [1700/1875], Loss: 0.0443\n",
            "Epoch [28/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [29/100], Step [100/1875], Loss: 0.0052\n",
            "Epoch [29/100], Step [200/1875], Loss: 0.0043\n",
            "Epoch [29/100], Step [300/1875], Loss: 0.0018\n",
            "Epoch [29/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [29/100], Step [500/1875], Loss: 0.0021\n",
            "Epoch [29/100], Step [600/1875], Loss: 0.1436\n",
            "Epoch [29/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [29/100], Step [800/1875], Loss: 0.0274\n",
            "Epoch [29/100], Step [900/1875], Loss: 0.0040\n",
            "Epoch [29/100], Step [1000/1875], Loss: 0.0086\n",
            "Epoch [29/100], Step [1100/1875], Loss: 0.0014\n",
            "Epoch [29/100], Step [1200/1875], Loss: 0.0308\n",
            "Epoch [29/100], Step [1300/1875], Loss: 0.0003\n",
            "Epoch [29/100], Step [1400/1875], Loss: 0.0010\n",
            "Epoch [29/100], Step [1500/1875], Loss: 0.0008\n",
            "Epoch [29/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [29/100], Step [1700/1875], Loss: 0.0063\n",
            "Epoch [29/100], Step [1800/1875], Loss: 0.0008\n",
            "Epoch [30/100], Step [100/1875], Loss: 0.1875\n",
            "Epoch [30/100], Step [200/1875], Loss: 0.0177\n",
            "Epoch [30/100], Step [300/1875], Loss: 0.0099\n",
            "Epoch [30/100], Step [400/1875], Loss: 0.0003\n",
            "Epoch [30/100], Step [500/1875], Loss: 0.0006\n",
            "Epoch [30/100], Step [600/1875], Loss: 0.0004\n",
            "Epoch [30/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [30/100], Step [800/1875], Loss: 0.0074\n",
            "Epoch [30/100], Step [900/1875], Loss: 0.0019\n",
            "Epoch [30/100], Step [1000/1875], Loss: 0.0027\n",
            "Epoch [30/100], Step [1100/1875], Loss: 0.0313\n",
            "Epoch [30/100], Step [1200/1875], Loss: 0.0215\n",
            "Epoch [30/100], Step [1300/1875], Loss: 0.0196\n",
            "Epoch [30/100], Step [1400/1875], Loss: 0.0004\n",
            "Epoch [30/100], Step [1500/1875], Loss: 0.0021\n",
            "Epoch [30/100], Step [1600/1875], Loss: 0.0731\n",
            "Epoch [30/100], Step [1700/1875], Loss: 0.0006\n",
            "Epoch [30/100], Step [1800/1875], Loss: 0.0009\n",
            "Epoch [31/100], Step [100/1875], Loss: 0.0010\n",
            "Epoch [31/100], Step [200/1875], Loss: 0.0457\n",
            "Epoch [31/100], Step [300/1875], Loss: 0.0065\n",
            "Epoch [31/100], Step [400/1875], Loss: 0.1277\n",
            "Epoch [31/100], Step [500/1875], Loss: 0.0015\n",
            "Epoch [31/100], Step [600/1875], Loss: 0.0105\n",
            "Epoch [31/100], Step [700/1875], Loss: 0.0043\n",
            "Epoch [31/100], Step [800/1875], Loss: 0.0006\n",
            "Epoch [31/100], Step [900/1875], Loss: 0.0002\n",
            "Epoch [31/100], Step [1000/1875], Loss: 0.0002\n",
            "Epoch [31/100], Step [1100/1875], Loss: 0.0028\n",
            "Epoch [31/100], Step [1200/1875], Loss: 0.0153\n",
            "Epoch [31/100], Step [1300/1875], Loss: 0.0006\n",
            "Epoch [31/100], Step [1400/1875], Loss: 0.1896\n",
            "Epoch [31/100], Step [1500/1875], Loss: 0.0199\n",
            "Epoch [31/100], Step [1600/1875], Loss: 0.0027\n",
            "Epoch [31/100], Step [1700/1875], Loss: 0.0038\n",
            "Epoch [31/100], Step [1800/1875], Loss: 0.0558\n",
            "Epoch [32/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [32/100], Step [200/1875], Loss: 0.0010\n",
            "Epoch [32/100], Step [300/1875], Loss: 0.0663\n",
            "Epoch [32/100], Step [400/1875], Loss: 0.1715\n",
            "Epoch [32/100], Step [500/1875], Loss: 0.0003\n",
            "Epoch [32/100], Step [600/1875], Loss: 0.0010\n",
            "Epoch [32/100], Step [700/1875], Loss: 0.0948\n",
            "Epoch [32/100], Step [800/1875], Loss: 0.0017\n",
            "Epoch [32/100], Step [900/1875], Loss: 0.0011\n",
            "Epoch [32/100], Step [1000/1875], Loss: 0.0128\n",
            "Epoch [32/100], Step [1100/1875], Loss: 0.0010\n",
            "Epoch [32/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [32/100], Step [1300/1875], Loss: 0.0009\n",
            "Epoch [32/100], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [32/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [32/100], Step [1600/1875], Loss: 0.1053\n",
            "Epoch [32/100], Step [1700/1875], Loss: 0.0002\n",
            "Epoch [32/100], Step [1800/1875], Loss: 0.0004\n",
            "Epoch [33/100], Step [100/1875], Loss: 0.0006\n",
            "Epoch [33/100], Step [200/1875], Loss: 0.0013\n",
            "Epoch [33/100], Step [300/1875], Loss: 0.0445\n",
            "Epoch [33/100], Step [400/1875], Loss: 0.0008\n",
            "Epoch [33/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [33/100], Step [600/1875], Loss: 0.0009\n",
            "Epoch [33/100], Step [700/1875], Loss: 0.0007\n",
            "Epoch [33/100], Step [800/1875], Loss: 0.0386\n",
            "Epoch [33/100], Step [900/1875], Loss: 0.0030\n",
            "Epoch [33/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [33/100], Step [1100/1875], Loss: 0.0004\n",
            "Epoch [33/100], Step [1200/1875], Loss: 0.0024\n",
            "Epoch [33/100], Step [1300/1875], Loss: 0.0009\n",
            "Epoch [33/100], Step [1400/1875], Loss: 0.4214\n",
            "Epoch [33/100], Step [1500/1875], Loss: 0.0018\n",
            "Epoch [33/100], Step [1600/1875], Loss: 0.0010\n",
            "Epoch [33/100], Step [1700/1875], Loss: 0.0015\n",
            "Epoch [33/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [34/100], Step [100/1875], Loss: 0.0008\n",
            "Epoch [34/100], Step [200/1875], Loss: 0.0046\n",
            "Epoch [34/100], Step [300/1875], Loss: 0.0030\n",
            "Epoch [34/100], Step [400/1875], Loss: 0.0006\n",
            "Epoch [34/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [34/100], Step [600/1875], Loss: 0.0015\n",
            "Epoch [34/100], Step [700/1875], Loss: 0.0003\n",
            "Epoch [34/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [34/100], Step [900/1875], Loss: 0.0924\n",
            "Epoch [34/100], Step [1000/1875], Loss: 0.1172\n",
            "Epoch [34/100], Step [1100/1875], Loss: 0.0003\n",
            "Epoch [34/100], Step [1200/1875], Loss: 0.0907\n",
            "Epoch [34/100], Step [1300/1875], Loss: 0.0014\n",
            "Epoch [34/100], Step [1400/1875], Loss: 0.0003\n",
            "Epoch [34/100], Step [1500/1875], Loss: 0.0019\n",
            "Epoch [34/100], Step [1600/1875], Loss: 0.0068\n",
            "Epoch [34/100], Step [1700/1875], Loss: 0.0005\n",
            "Epoch [34/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [35/100], Step [100/1875], Loss: 0.0257\n",
            "Epoch [35/100], Step [200/1875], Loss: 0.0127\n",
            "Epoch [35/100], Step [300/1875], Loss: 0.0007\n",
            "Epoch [35/100], Step [400/1875], Loss: 0.0004\n",
            "Epoch [35/100], Step [500/1875], Loss: 0.0094\n",
            "Epoch [35/100], Step [600/1875], Loss: 0.0015\n",
            "Epoch [35/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [35/100], Step [800/1875], Loss: 0.0055\n",
            "Epoch [35/100], Step [900/1875], Loss: 0.0034\n",
            "Epoch [35/100], Step [1000/1875], Loss: 0.0017\n",
            "Epoch [35/100], Step [1100/1875], Loss: 0.0609\n",
            "Epoch [35/100], Step [1200/1875], Loss: 0.1207\n",
            "Epoch [35/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [35/100], Step [1400/1875], Loss: 0.0081\n",
            "Epoch [35/100], Step [1500/1875], Loss: 0.0012\n",
            "Epoch [35/100], Step [1600/1875], Loss: 0.0802\n",
            "Epoch [35/100], Step [1700/1875], Loss: 0.0237\n",
            "Epoch [35/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [36/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [36/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [36/100], Step [300/1875], Loss: 0.0007\n",
            "Epoch [36/100], Step [400/1875], Loss: 0.1341\n",
            "Epoch [36/100], Step [500/1875], Loss: 0.0007\n",
            "Epoch [36/100], Step [600/1875], Loss: 0.0004\n",
            "Epoch [36/100], Step [700/1875], Loss: 0.0005\n",
            "Epoch [36/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [36/100], Step [900/1875], Loss: 0.0051\n",
            "Epoch [36/100], Step [1000/1875], Loss: 0.0023\n",
            "Epoch [36/100], Step [1100/1875], Loss: 0.0003\n",
            "Epoch [36/100], Step [1200/1875], Loss: 0.0006\n",
            "Epoch [36/100], Step [1300/1875], Loss: 0.0036\n",
            "Epoch [36/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [36/100], Step [1500/1875], Loss: 0.0286\n",
            "Epoch [36/100], Step [1600/1875], Loss: 0.0002\n",
            "Epoch [36/100], Step [1700/1875], Loss: 0.0005\n",
            "Epoch [36/100], Step [1800/1875], Loss: 0.0085\n",
            "Epoch [37/100], Step [100/1875], Loss: 0.0004\n",
            "Epoch [37/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [37/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [37/100], Step [400/1875], Loss: 0.0007\n",
            "Epoch [37/100], Step [500/1875], Loss: 0.0003\n",
            "Epoch [37/100], Step [600/1875], Loss: 0.0254\n",
            "Epoch [37/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [37/100], Step [800/1875], Loss: 0.0027\n",
            "Epoch [37/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [37/100], Step [1000/1875], Loss: 0.0018\n",
            "Epoch [37/100], Step [1100/1875], Loss: 0.0004\n",
            "Epoch [37/100], Step [1200/1875], Loss: 0.0021\n",
            "Epoch [37/100], Step [1300/1875], Loss: 0.0033\n",
            "Epoch [37/100], Step [1400/1875], Loss: 0.0862\n",
            "Epoch [37/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [37/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [37/100], Step [1700/1875], Loss: 0.0004\n",
            "Epoch [37/100], Step [1800/1875], Loss: 0.0150\n",
            "Epoch [38/100], Step [100/1875], Loss: 0.0022\n",
            "Epoch [38/100], Step [200/1875], Loss: 0.0002\n",
            "Epoch [38/100], Step [300/1875], Loss: 0.0005\n",
            "Epoch [38/100], Step [400/1875], Loss: 0.0137\n",
            "Epoch [38/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [38/100], Step [600/1875], Loss: 0.0025\n",
            "Epoch [38/100], Step [700/1875], Loss: 0.0007\n",
            "Epoch [38/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [38/100], Step [900/1875], Loss: 0.0028\n",
            "Epoch [38/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [38/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [38/100], Step [1200/1875], Loss: 0.0666\n",
            "Epoch [38/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [38/100], Step [1400/1875], Loss: 0.0017\n",
            "Epoch [38/100], Step [1500/1875], Loss: 0.0150\n",
            "Epoch [38/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [38/100], Step [1700/1875], Loss: 0.0388\n",
            "Epoch [38/100], Step [1800/1875], Loss: 0.0047\n",
            "Epoch [39/100], Step [100/1875], Loss: 0.0006\n",
            "Epoch [39/100], Step [200/1875], Loss: 0.0216\n",
            "Epoch [39/100], Step [300/1875], Loss: 0.0448\n",
            "Epoch [39/100], Step [400/1875], Loss: 0.0009\n",
            "Epoch [39/100], Step [500/1875], Loss: 0.0080\n",
            "Epoch [39/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [39/100], Step [700/1875], Loss: 0.0008\n",
            "Epoch [39/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [39/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [39/100], Step [1000/1875], Loss: 0.1539\n",
            "Epoch [39/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [39/100], Step [1200/1875], Loss: 0.0011\n",
            "Epoch [39/100], Step [1300/1875], Loss: 0.0008\n",
            "Epoch [39/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [39/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [39/100], Step [1600/1875], Loss: 0.0130\n",
            "Epoch [39/100], Step [1700/1875], Loss: 0.0003\n",
            "Epoch [39/100], Step [1800/1875], Loss: 0.0306\n",
            "Epoch [40/100], Step [100/1875], Loss: 0.0004\n",
            "Epoch [40/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [40/100], Step [300/1875], Loss: 0.0520\n",
            "Epoch [40/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [40/100], Step [500/1875], Loss: 0.0064\n",
            "Epoch [40/100], Step [600/1875], Loss: 0.0097\n",
            "Epoch [40/100], Step [700/1875], Loss: 0.0208\n",
            "Epoch [40/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [40/100], Step [900/1875], Loss: 0.0052\n",
            "Epoch [40/100], Step [1000/1875], Loss: 0.0693\n",
            "Epoch [40/100], Step [1100/1875], Loss: 0.0005\n",
            "Epoch [40/100], Step [1200/1875], Loss: 0.0158\n",
            "Epoch [40/100], Step [1300/1875], Loss: 0.0066\n",
            "Epoch [40/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [40/100], Step [1500/1875], Loss: 0.0004\n",
            "Epoch [40/100], Step [1600/1875], Loss: 0.0476\n",
            "Epoch [40/100], Step [1700/1875], Loss: 0.0288\n",
            "Epoch [40/100], Step [1800/1875], Loss: 0.0008\n",
            "Epoch [41/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [41/100], Step [200/1875], Loss: 0.0053\n",
            "Epoch [41/100], Step [300/1875], Loss: 0.0007\n",
            "Epoch [41/100], Step [400/1875], Loss: 0.0004\n",
            "Epoch [41/100], Step [500/1875], Loss: 0.0009\n",
            "Epoch [41/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [41/100], Step [700/1875], Loss: 0.0130\n",
            "Epoch [41/100], Step [800/1875], Loss: 0.0012\n",
            "Epoch [41/100], Step [900/1875], Loss: 0.1318\n",
            "Epoch [41/100], Step [1000/1875], Loss: 0.0425\n",
            "Epoch [41/100], Step [1100/1875], Loss: 0.0006\n",
            "Epoch [41/100], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [41/100], Step [1300/1875], Loss: 0.0034\n",
            "Epoch [41/100], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [41/100], Step [1500/1875], Loss: 0.0075\n",
            "Epoch [41/100], Step [1600/1875], Loss: 0.0011\n",
            "Epoch [41/100], Step [1700/1875], Loss: 0.0012\n",
            "Epoch [41/100], Step [1800/1875], Loss: 0.0295\n",
            "Epoch [42/100], Step [100/1875], Loss: 0.0027\n",
            "Epoch [42/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [42/100], Step [500/1875], Loss: 0.0007\n",
            "Epoch [42/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [700/1875], Loss: 0.0186\n",
            "Epoch [42/100], Step [800/1875], Loss: 0.0175\n",
            "Epoch [42/100], Step [900/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [1200/1875], Loss: 0.0066\n",
            "Epoch [42/100], Step [1300/1875], Loss: 0.0020\n",
            "Epoch [42/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [42/100], Step [1500/1875], Loss: 0.0042\n",
            "Epoch [42/100], Step [1600/1875], Loss: 0.0081\n",
            "Epoch [42/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [42/100], Step [1800/1875], Loss: 0.0121\n",
            "Epoch [43/100], Step [100/1875], Loss: 0.0017\n",
            "Epoch [43/100], Step [200/1875], Loss: 0.0008\n",
            "Epoch [43/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [43/100], Step [400/1875], Loss: 0.0023\n",
            "Epoch [43/100], Step [500/1875], Loss: 0.0013\n",
            "Epoch [43/100], Step [600/1875], Loss: 0.0003\n",
            "Epoch [43/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [43/100], Step [800/1875], Loss: 0.0004\n",
            "Epoch [43/100], Step [900/1875], Loss: 0.0227\n",
            "Epoch [43/100], Step [1000/1875], Loss: 0.0005\n",
            "Epoch [43/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [43/100], Step [1200/1875], Loss: 0.0007\n",
            "Epoch [43/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [43/100], Step [1400/1875], Loss: 0.0004\n",
            "Epoch [43/100], Step [1500/1875], Loss: 0.0007\n",
            "Epoch [43/100], Step [1600/1875], Loss: 0.0002\n",
            "Epoch [43/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [43/100], Step [1800/1875], Loss: 0.0004\n",
            "Epoch [44/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [44/100], Step [200/1875], Loss: 0.0099\n",
            "Epoch [44/100], Step [300/1875], Loss: 0.0189\n",
            "Epoch [44/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [44/100], Step [500/1875], Loss: 0.0029\n",
            "Epoch [44/100], Step [600/1875], Loss: 0.0019\n",
            "Epoch [44/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [44/100], Step [800/1875], Loss: 0.0043\n",
            "Epoch [44/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [44/100], Step [1000/1875], Loss: 0.0028\n",
            "Epoch [44/100], Step [1100/1875], Loss: 0.0021\n",
            "Epoch [44/100], Step [1200/1875], Loss: 0.0023\n",
            "Epoch [44/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [44/100], Step [1400/1875], Loss: 0.0004\n",
            "Epoch [44/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [44/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [44/100], Step [1700/1875], Loss: 0.0026\n",
            "Epoch [44/100], Step [1800/1875], Loss: 0.0029\n",
            "Epoch [45/100], Step [100/1875], Loss: 0.0005\n",
            "Epoch [45/100], Step [200/1875], Loss: 0.0962\n",
            "Epoch [45/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [45/100], Step [400/1875], Loss: 0.0210\n",
            "Epoch [45/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [45/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [45/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [45/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [45/100], Step [900/1875], Loss: 0.0009\n",
            "Epoch [45/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [45/100], Step [1100/1875], Loss: 0.0180\n",
            "Epoch [45/100], Step [1200/1875], Loss: 0.0119\n",
            "Epoch [45/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [45/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [45/100], Step [1500/1875], Loss: 0.0708\n",
            "Epoch [45/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [45/100], Step [1700/1875], Loss: 0.0002\n",
            "Epoch [45/100], Step [1800/1875], Loss: 0.0128\n",
            "Epoch [46/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [46/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [46/100], Step [300/1875], Loss: 0.0047\n",
            "Epoch [46/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [46/100], Step [500/1875], Loss: 0.0106\n",
            "Epoch [46/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [46/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [46/100], Step [800/1875], Loss: 0.0008\n",
            "Epoch [46/100], Step [900/1875], Loss: 0.0007\n",
            "Epoch [46/100], Step [1000/1875], Loss: 0.0243\n",
            "Epoch [46/100], Step [1100/1875], Loss: 0.0002\n",
            "Epoch [46/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [46/100], Step [1300/1875], Loss: 0.0129\n",
            "Epoch [46/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [46/100], Step [1500/1875], Loss: 0.0007\n",
            "Epoch [46/100], Step [1600/1875], Loss: 0.0012\n",
            "Epoch [46/100], Step [1700/1875], Loss: 0.0018\n",
            "Epoch [46/100], Step [1800/1875], Loss: 0.0064\n",
            "Epoch [47/100], Step [100/1875], Loss: 0.0023\n",
            "Epoch [47/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [47/100], Step [300/1875], Loss: 0.0029\n",
            "Epoch [47/100], Step [400/1875], Loss: 0.1689\n",
            "Epoch [47/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [47/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [47/100], Step [700/1875], Loss: 0.0042\n",
            "Epoch [47/100], Step [800/1875], Loss: 0.0008\n",
            "Epoch [47/100], Step [900/1875], Loss: 0.0097\n",
            "Epoch [47/100], Step [1000/1875], Loss: 0.0095\n",
            "Epoch [47/100], Step [1100/1875], Loss: 0.1028\n",
            "Epoch [47/100], Step [1200/1875], Loss: 0.0574\n",
            "Epoch [47/100], Step [1300/1875], Loss: 0.0064\n",
            "Epoch [47/100], Step [1400/1875], Loss: 0.0054\n",
            "Epoch [47/100], Step [1500/1875], Loss: 0.0029\n",
            "Epoch [47/100], Step [1600/1875], Loss: 0.0160\n",
            "Epoch [47/100], Step [1700/1875], Loss: 0.0859\n",
            "Epoch [47/100], Step [1800/1875], Loss: 0.0017\n",
            "Epoch [48/100], Step [100/1875], Loss: 0.0397\n",
            "Epoch [48/100], Step [200/1875], Loss: 0.0135\n",
            "Epoch [48/100], Step [300/1875], Loss: 0.0351\n",
            "Epoch [48/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [48/100], Step [500/1875], Loss: 0.0005\n",
            "Epoch [48/100], Step [600/1875], Loss: 0.0004\n",
            "Epoch [48/100], Step [700/1875], Loss: 0.0016\n",
            "Epoch [48/100], Step [800/1875], Loss: 0.0009\n",
            "Epoch [48/100], Step [900/1875], Loss: 0.0292\n",
            "Epoch [48/100], Step [1000/1875], Loss: 0.3230\n",
            "Epoch [48/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [48/100], Step [1200/1875], Loss: 0.0003\n",
            "Epoch [48/100], Step [1300/1875], Loss: 0.0149\n",
            "Epoch [48/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [48/100], Step [1500/1875], Loss: 0.0272\n",
            "Epoch [48/100], Step [1600/1875], Loss: 0.0031\n",
            "Epoch [48/100], Step [1700/1875], Loss: 0.0011\n",
            "Epoch [48/100], Step [1800/1875], Loss: 0.0113\n",
            "Epoch [49/100], Step [100/1875], Loss: 0.0004\n",
            "Epoch [49/100], Step [200/1875], Loss: 0.0005\n",
            "Epoch [49/100], Step [300/1875], Loss: 0.0132\n",
            "Epoch [49/100], Step [400/1875], Loss: 0.0009\n",
            "Epoch [49/100], Step [500/1875], Loss: 0.0054\n",
            "Epoch [49/100], Step [600/1875], Loss: 0.0017\n",
            "Epoch [49/100], Step [700/1875], Loss: 0.0056\n",
            "Epoch [49/100], Step [800/1875], Loss: 0.0004\n",
            "Epoch [49/100], Step [900/1875], Loss: 0.0012\n",
            "Epoch [49/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [49/100], Step [1100/1875], Loss: 0.0163\n",
            "Epoch [49/100], Step [1200/1875], Loss: 0.0036\n",
            "Epoch [49/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [49/100], Step [1400/1875], Loss: 0.0021\n",
            "Epoch [49/100], Step [1500/1875], Loss: 0.0022\n",
            "Epoch [49/100], Step [1600/1875], Loss: 0.0151\n",
            "Epoch [49/100], Step [1700/1875], Loss: 0.0006\n",
            "Epoch [49/100], Step [1800/1875], Loss: 0.0104\n",
            "Epoch [50/100], Step [100/1875], Loss: 0.0010\n",
            "Epoch [50/100], Step [200/1875], Loss: 0.7865\n",
            "Epoch [50/100], Step [300/1875], Loss: 0.5471\n",
            "Epoch [50/100], Step [400/1875], Loss: 0.1848\n",
            "Epoch [50/100], Step [500/1875], Loss: 0.0514\n",
            "Epoch [50/100], Step [600/1875], Loss: 0.0817\n",
            "Epoch [50/100], Step [700/1875], Loss: 0.0186\n",
            "Epoch [50/100], Step [800/1875], Loss: 0.0004\n",
            "Epoch [50/100], Step [900/1875], Loss: 0.0037\n",
            "Epoch [50/100], Step [1000/1875], Loss: 0.0064\n",
            "Epoch [50/100], Step [1100/1875], Loss: 0.0183\n",
            "Epoch [50/100], Step [1200/1875], Loss: 0.1200\n",
            "Epoch [50/100], Step [1300/1875], Loss: 0.0057\n",
            "Epoch [50/100], Step [1400/1875], Loss: 0.0184\n",
            "Epoch [50/100], Step [1500/1875], Loss: 0.0007\n",
            "Epoch [50/100], Step [1600/1875], Loss: 0.0087\n",
            "Epoch [50/100], Step [1700/1875], Loss: 0.0332\n",
            "Epoch [50/100], Step [1800/1875], Loss: 0.0067\n",
            "Epoch [51/100], Step [100/1875], Loss: 0.0015\n",
            "Epoch [51/100], Step [200/1875], Loss: 0.0091\n",
            "Epoch [51/100], Step [300/1875], Loss: 0.0005\n",
            "Epoch [51/100], Step [400/1875], Loss: 0.0096\n",
            "Epoch [51/100], Step [500/1875], Loss: 0.0245\n",
            "Epoch [51/100], Step [600/1875], Loss: 0.0041\n",
            "Epoch [51/100], Step [700/1875], Loss: 0.1459\n",
            "Epoch [51/100], Step [800/1875], Loss: 0.0164\n",
            "Epoch [51/100], Step [900/1875], Loss: 0.0278\n",
            "Epoch [51/100], Step [1000/1875], Loss: 0.0045\n",
            "Epoch [51/100], Step [1100/1875], Loss: 0.0007\n",
            "Epoch [51/100], Step [1200/1875], Loss: 0.0541\n",
            "Epoch [51/100], Step [1300/1875], Loss: 0.0086\n",
            "Epoch [51/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [51/100], Step [1500/1875], Loss: 0.0162\n",
            "Epoch [51/100], Step [1600/1875], Loss: 0.0336\n",
            "Epoch [51/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [51/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [52/100], Step [100/1875], Loss: 0.0013\n",
            "Epoch [52/100], Step [200/1875], Loss: 0.0004\n",
            "Epoch [52/100], Step [300/1875], Loss: 0.0003\n",
            "Epoch [52/100], Step [400/1875], Loss: 0.0672\n",
            "Epoch [52/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [52/100], Step [600/1875], Loss: 0.0002\n",
            "Epoch [52/100], Step [700/1875], Loss: 0.0010\n",
            "Epoch [52/100], Step [800/1875], Loss: 0.0007\n",
            "Epoch [52/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [52/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [52/100], Step [1100/1875], Loss: 0.0002\n",
            "Epoch [52/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [52/100], Step [1300/1875], Loss: 0.0244\n",
            "Epoch [52/100], Step [1400/1875], Loss: 0.0015\n",
            "Epoch [52/100], Step [1500/1875], Loss: 0.0042\n",
            "Epoch [52/100], Step [1600/1875], Loss: 0.0033\n",
            "Epoch [52/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [52/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [53/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [53/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [53/100], Step [300/1875], Loss: 0.0008\n",
            "Epoch [53/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [53/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [53/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [53/100], Step [700/1875], Loss: 0.0064\n",
            "Epoch [53/100], Step [800/1875], Loss: 0.0054\n",
            "Epoch [53/100], Step [900/1875], Loss: 0.0008\n",
            "Epoch [53/100], Step [1000/1875], Loss: 0.0006\n",
            "Epoch [53/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [53/100], Step [1200/1875], Loss: 0.0021\n",
            "Epoch [53/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [53/100], Step [1400/1875], Loss: 0.0036\n",
            "Epoch [53/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [53/100], Step [1600/1875], Loss: 0.0036\n",
            "Epoch [53/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [53/100], Step [1800/1875], Loss: 0.0283\n",
            "Epoch [54/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [54/100], Step [200/1875], Loss: 0.0006\n",
            "Epoch [54/100], Step [300/1875], Loss: 0.0052\n",
            "Epoch [54/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [54/100], Step [500/1875], Loss: 0.0830\n",
            "Epoch [54/100], Step [600/1875], Loss: 0.0026\n",
            "Epoch [54/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [54/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [54/100], Step [900/1875], Loss: 0.0327\n",
            "Epoch [54/100], Step [1000/1875], Loss: 0.0044\n",
            "Epoch [54/100], Step [1100/1875], Loss: 0.0052\n",
            "Epoch [54/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [54/100], Step [1300/1875], Loss: 0.0007\n",
            "Epoch [54/100], Step [1400/1875], Loss: 0.0242\n",
            "Epoch [54/100], Step [1500/1875], Loss: 0.0002\n",
            "Epoch [54/100], Step [1600/1875], Loss: 0.0014\n",
            "Epoch [54/100], Step [1700/1875], Loss: 0.0005\n",
            "Epoch [54/100], Step [1800/1875], Loss: 0.0413\n",
            "Epoch [55/100], Step [100/1875], Loss: 0.0351\n",
            "Epoch [55/100], Step [200/1875], Loss: 0.0002\n",
            "Epoch [55/100], Step [300/1875], Loss: 0.0014\n",
            "Epoch [55/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [55/100], Step [500/1875], Loss: 0.0005\n",
            "Epoch [55/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [55/100], Step [700/1875], Loss: 0.0005\n",
            "Epoch [55/100], Step [800/1875], Loss: 0.0016\n",
            "Epoch [55/100], Step [900/1875], Loss: 0.0160\n",
            "Epoch [55/100], Step [1000/1875], Loss: 0.0080\n",
            "Epoch [55/100], Step [1100/1875], Loss: 0.0108\n",
            "Epoch [55/100], Step [1200/1875], Loss: 0.0327\n",
            "Epoch [55/100], Step [1300/1875], Loss: 0.0391\n",
            "Epoch [55/100], Step [1400/1875], Loss: 0.0209\n",
            "Epoch [55/100], Step [1500/1875], Loss: 0.0017\n",
            "Epoch [55/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [55/100], Step [1700/1875], Loss: 0.0015\n",
            "Epoch [55/100], Step [1800/1875], Loss: 0.1177\n",
            "Epoch [56/100], Step [100/1875], Loss: 0.0004\n",
            "Epoch [56/100], Step [200/1875], Loss: 0.0109\n",
            "Epoch [56/100], Step [300/1875], Loss: 0.0010\n",
            "Epoch [56/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [56/100], Step [500/1875], Loss: 0.0003\n",
            "Epoch [56/100], Step [600/1875], Loss: 0.0047\n",
            "Epoch [56/100], Step [700/1875], Loss: 0.0041\n",
            "Epoch [56/100], Step [800/1875], Loss: 0.0022\n",
            "Epoch [56/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [56/100], Step [1000/1875], Loss: 0.0003\n",
            "Epoch [56/100], Step [1100/1875], Loss: 0.0005\n",
            "Epoch [56/100], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [56/100], Step [1300/1875], Loss: 0.0003\n",
            "Epoch [56/100], Step [1400/1875], Loss: 0.0003\n",
            "Epoch [56/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [56/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [56/100], Step [1700/1875], Loss: 0.0008\n",
            "Epoch [56/100], Step [1800/1875], Loss: 0.0006\n",
            "Epoch [57/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [57/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [57/100], Step [300/1875], Loss: 0.0014\n",
            "Epoch [57/100], Step [400/1875], Loss: 0.0018\n",
            "Epoch [57/100], Step [500/1875], Loss: 0.0008\n",
            "Epoch [57/100], Step [600/1875], Loss: 0.0051\n",
            "Epoch [57/100], Step [700/1875], Loss: 0.0004\n",
            "Epoch [57/100], Step [800/1875], Loss: 0.0011\n",
            "Epoch [57/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [57/100], Step [1000/1875], Loss: 0.0117\n",
            "Epoch [57/100], Step [1100/1875], Loss: 0.0093\n",
            "Epoch [57/100], Step [1200/1875], Loss: 0.1196\n",
            "Epoch [57/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [57/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [57/100], Step [1500/1875], Loss: 0.0141\n",
            "Epoch [57/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [57/100], Step [1700/1875], Loss: 0.0211\n",
            "Epoch [57/100], Step [1800/1875], Loss: 0.0024\n",
            "Epoch [58/100], Step [100/1875], Loss: 0.0014\n",
            "Epoch [58/100], Step [200/1875], Loss: 0.0007\n",
            "Epoch [58/100], Step [300/1875], Loss: 0.0003\n",
            "Epoch [58/100], Step [400/1875], Loss: 0.0004\n",
            "Epoch [58/100], Step [500/1875], Loss: 0.0099\n",
            "Epoch [58/100], Step [600/1875], Loss: 0.0013\n",
            "Epoch [58/100], Step [700/1875], Loss: 0.0035\n",
            "Epoch [58/100], Step [800/1875], Loss: 0.0002\n",
            "Epoch [58/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [58/100], Step [1000/1875], Loss: 0.0010\n",
            "Epoch [58/100], Step [1100/1875], Loss: 0.0005\n",
            "Epoch [58/100], Step [1200/1875], Loss: 0.0007\n",
            "Epoch [58/100], Step [1300/1875], Loss: 0.0037\n",
            "Epoch [58/100], Step [1400/1875], Loss: 0.0237\n",
            "Epoch [58/100], Step [1500/1875], Loss: 0.0003\n",
            "Epoch [58/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [58/100], Step [1700/1875], Loss: 0.0002\n",
            "Epoch [58/100], Step [1800/1875], Loss: 0.2200\n",
            "Epoch [59/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [59/100], Step [200/1875], Loss: 0.0056\n",
            "Epoch [59/100], Step [300/1875], Loss: 0.0007\n",
            "Epoch [59/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [59/100], Step [500/1875], Loss: 0.0004\n",
            "Epoch [59/100], Step [600/1875], Loss: 0.0008\n",
            "Epoch [59/100], Step [700/1875], Loss: 0.0304\n",
            "Epoch [59/100], Step [800/1875], Loss: 0.0002\n",
            "Epoch [59/100], Step [900/1875], Loss: 0.2422\n",
            "Epoch [59/100], Step [1000/1875], Loss: 0.0003\n",
            "Epoch [59/100], Step [1100/1875], Loss: 0.0796\n",
            "Epoch [59/100], Step [1200/1875], Loss: 0.0007\n",
            "Epoch [59/100], Step [1300/1875], Loss: 0.0005\n",
            "Epoch [59/100], Step [1400/1875], Loss: 0.0012\n",
            "Epoch [59/100], Step [1500/1875], Loss: 0.0004\n",
            "Epoch [59/100], Step [1600/1875], Loss: 0.0275\n",
            "Epoch [59/100], Step [1700/1875], Loss: 0.0007\n",
            "Epoch [59/100], Step [1800/1875], Loss: 0.0004\n",
            "Epoch [60/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [60/100], Step [200/1875], Loss: 0.0002\n",
            "Epoch [60/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [60/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [60/100], Step [500/1875], Loss: 0.0005\n",
            "Epoch [60/100], Step [600/1875], Loss: 0.0002\n",
            "Epoch [60/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [60/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [60/100], Step [900/1875], Loss: 0.0444\n",
            "Epoch [60/100], Step [1000/1875], Loss: 0.0005\n",
            "Epoch [60/100], Step [1100/1875], Loss: 0.0002\n",
            "Epoch [60/100], Step [1200/1875], Loss: 0.0069\n",
            "Epoch [60/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [60/100], Step [1400/1875], Loss: 0.0012\n",
            "Epoch [60/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [60/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [60/100], Step [1700/1875], Loss: 0.0003\n",
            "Epoch [60/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [61/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [61/100], Step [200/1875], Loss: 0.0004\n",
            "Epoch [61/100], Step [300/1875], Loss: 0.0337\n",
            "Epoch [61/100], Step [400/1875], Loss: 0.0008\n",
            "Epoch [61/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [61/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [61/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [61/100], Step [800/1875], Loss: 0.0364\n",
            "Epoch [61/100], Step [900/1875], Loss: 0.2484\n",
            "Epoch [61/100], Step [1000/1875], Loss: 0.0300\n",
            "Epoch [61/100], Step [1100/1875], Loss: 0.0083\n",
            "Epoch [61/100], Step [1200/1875], Loss: 0.0073\n",
            "Epoch [61/100], Step [1300/1875], Loss: 0.0107\n",
            "Epoch [61/100], Step [1400/1875], Loss: 0.0024\n",
            "Epoch [61/100], Step [1500/1875], Loss: 0.0270\n",
            "Epoch [61/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [61/100], Step [1700/1875], Loss: 0.0018\n",
            "Epoch [61/100], Step [1800/1875], Loss: 0.0003\n",
            "Epoch [62/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [62/100], Step [200/1875], Loss: 0.0691\n",
            "Epoch [62/100], Step [300/1875], Loss: 0.0004\n",
            "Epoch [62/100], Step [400/1875], Loss: 0.0022\n",
            "Epoch [62/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [62/100], Step [600/1875], Loss: 0.0012\n",
            "Epoch [62/100], Step [700/1875], Loss: 0.0003\n",
            "Epoch [62/100], Step [800/1875], Loss: 0.0002\n",
            "Epoch [62/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [62/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [62/100], Step [1100/1875], Loss: 0.0007\n",
            "Epoch [62/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [62/100], Step [1300/1875], Loss: 0.0430\n",
            "Epoch [62/100], Step [1400/1875], Loss: 0.0506\n",
            "Epoch [62/100], Step [1500/1875], Loss: 0.0010\n",
            "Epoch [62/100], Step [1600/1875], Loss: 0.0022\n",
            "Epoch [62/100], Step [1700/1875], Loss: 0.0022\n",
            "Epoch [62/100], Step [1800/1875], Loss: 0.0003\n",
            "Epoch [63/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [63/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [63/100], Step [300/1875], Loss: 0.0034\n",
            "Epoch [63/100], Step [400/1875], Loss: 0.0012\n",
            "Epoch [63/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [63/100], Step [600/1875], Loss: 0.0002\n",
            "Epoch [63/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [63/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [63/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [63/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [63/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [63/100], Step [1200/1875], Loss: 0.0002\n",
            "Epoch [63/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [63/100], Step [1400/1875], Loss: 0.0186\n",
            "Epoch [63/100], Step [1500/1875], Loss: 0.0008\n",
            "Epoch [63/100], Step [1600/1875], Loss: 0.0011\n",
            "Epoch [63/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [63/100], Step [1800/1875], Loss: 0.0003\n",
            "Epoch [64/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [64/100], Step [200/1875], Loss: 0.0002\n",
            "Epoch [64/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [64/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [64/100], Step [500/1875], Loss: 0.0059\n",
            "Epoch [64/100], Step [600/1875], Loss: 0.0103\n",
            "Epoch [64/100], Step [700/1875], Loss: 0.0004\n",
            "Epoch [64/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [64/100], Step [900/1875], Loss: 0.0000\n",
            "Epoch [64/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [64/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [64/100], Step [1200/1875], Loss: 0.0002\n",
            "Epoch [64/100], Step [1300/1875], Loss: 0.0006\n",
            "Epoch [64/100], Step [1400/1875], Loss: 0.0003\n",
            "Epoch [64/100], Step [1500/1875], Loss: 0.0008\n",
            "Epoch [64/100], Step [1600/1875], Loss: 0.0006\n",
            "Epoch [64/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [64/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [65/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [65/100], Step [200/1875], Loss: 0.0011\n",
            "Epoch [65/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [65/100], Step [400/1875], Loss: 0.0057\n",
            "Epoch [65/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [65/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [65/100], Step [700/1875], Loss: 0.0044\n",
            "Epoch [65/100], Step [800/1875], Loss: 0.0006\n",
            "Epoch [65/100], Step [900/1875], Loss: 0.0396\n",
            "Epoch [65/100], Step [1000/1875], Loss: 0.0003\n",
            "Epoch [65/100], Step [1100/1875], Loss: 0.0009\n",
            "Epoch [65/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [65/100], Step [1300/1875], Loss: 0.0003\n",
            "Epoch [65/100], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [65/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [65/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [65/100], Step [1700/1875], Loss: 0.0002\n",
            "Epoch [65/100], Step [1800/1875], Loss: 0.0024\n",
            "Epoch [66/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [66/100], Step [200/1875], Loss: 0.0013\n",
            "Epoch [66/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [66/100], Step [400/1875], Loss: 0.0120\n",
            "Epoch [66/100], Step [500/1875], Loss: 0.0027\n",
            "Epoch [66/100], Step [600/1875], Loss: 0.0012\n",
            "Epoch [66/100], Step [700/1875], Loss: 0.0085\n",
            "Epoch [66/100], Step [800/1875], Loss: 0.0120\n",
            "Epoch [66/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [66/100], Step [1000/1875], Loss: 0.0025\n",
            "Epoch [66/100], Step [1100/1875], Loss: 0.0026\n",
            "Epoch [66/100], Step [1200/1875], Loss: 0.0102\n",
            "Epoch [66/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [66/100], Step [1400/1875], Loss: 0.0003\n",
            "Epoch [66/100], Step [1500/1875], Loss: 0.0117\n",
            "Epoch [66/100], Step [1600/1875], Loss: 0.0007\n",
            "Epoch [66/100], Step [1700/1875], Loss: 0.0429\n",
            "Epoch [66/100], Step [1800/1875], Loss: 0.0168\n",
            "Epoch [67/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [67/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [67/100], Step [300/1875], Loss: 0.0010\n",
            "Epoch [67/100], Step [400/1875], Loss: 0.0132\n",
            "Epoch [67/100], Step [500/1875], Loss: 0.0455\n",
            "Epoch [67/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [67/100], Step [700/1875], Loss: 0.0003\n",
            "Epoch [67/100], Step [800/1875], Loss: 0.0004\n",
            "Epoch [67/100], Step [900/1875], Loss: 0.0008\n",
            "Epoch [67/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [67/100], Step [1100/1875], Loss: 0.0359\n",
            "Epoch [67/100], Step [1200/1875], Loss: 0.0006\n",
            "Epoch [67/100], Step [1300/1875], Loss: 0.0004\n",
            "Epoch [67/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [67/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [67/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [67/100], Step [1700/1875], Loss: 0.0473\n",
            "Epoch [67/100], Step [1800/1875], Loss: 0.0010\n",
            "Epoch [68/100], Step [100/1875], Loss: 0.0052\n",
            "Epoch [68/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [68/100], Step [300/1875], Loss: 0.0012\n",
            "Epoch [68/100], Step [400/1875], Loss: 0.0007\n",
            "Epoch [68/100], Step [500/1875], Loss: 0.1403\n",
            "Epoch [68/100], Step [600/1875], Loss: 0.0007\n",
            "Epoch [68/100], Step [700/1875], Loss: 0.0017\n",
            "Epoch [68/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [68/100], Step [900/1875], Loss: 0.0032\n",
            "Epoch [68/100], Step [1000/1875], Loss: 0.0023\n",
            "Epoch [68/100], Step [1100/1875], Loss: 0.0147\n",
            "Epoch [68/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [68/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [68/100], Step [1400/1875], Loss: 0.0004\n",
            "Epoch [68/100], Step [1500/1875], Loss: 0.0002\n",
            "Epoch [68/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [68/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [68/100], Step [1800/1875], Loss: 0.4230\n",
            "Epoch [69/100], Step [100/1875], Loss: 0.1571\n",
            "Epoch [69/100], Step [200/1875], Loss: 0.0299\n",
            "Epoch [69/100], Step [300/1875], Loss: 0.0012\n",
            "Epoch [69/100], Step [400/1875], Loss: 0.0025\n",
            "Epoch [69/100], Step [500/1875], Loss: 0.0003\n",
            "Epoch [69/100], Step [600/1875], Loss: 0.0002\n",
            "Epoch [69/100], Step [700/1875], Loss: 0.0011\n",
            "Epoch [69/100], Step [800/1875], Loss: 0.0068\n",
            "Epoch [69/100], Step [900/1875], Loss: 0.0035\n",
            "Epoch [69/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [69/100], Step [1100/1875], Loss: 0.0002\n",
            "Epoch [69/100], Step [1200/1875], Loss: 0.0002\n",
            "Epoch [69/100], Step [1300/1875], Loss: 0.0019\n",
            "Epoch [69/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [69/100], Step [1500/1875], Loss: 0.0033\n",
            "Epoch [69/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [69/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [69/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [70/100], Step [100/1875], Loss: 0.0016\n",
            "Epoch [70/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [70/100], Step [300/1875], Loss: 0.0012\n",
            "Epoch [70/100], Step [400/1875], Loss: 0.0007\n",
            "Epoch [70/100], Step [500/1875], Loss: 0.0033\n",
            "Epoch [70/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [70/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [70/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [70/100], Step [900/1875], Loss: 0.0006\n",
            "Epoch [70/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [70/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [70/100], Step [1200/1875], Loss: 0.0108\n",
            "Epoch [70/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [70/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [70/100], Step [1500/1875], Loss: 0.0019\n",
            "Epoch [70/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [70/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [70/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [71/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [71/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [71/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [71/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [71/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [71/100], Step [600/1875], Loss: 0.0194\n",
            "Epoch [71/100], Step [700/1875], Loss: 0.0006\n",
            "Epoch [71/100], Step [800/1875], Loss: 0.0418\n",
            "Epoch [71/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [71/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [71/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [71/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [71/100], Step [1300/1875], Loss: 0.0007\n",
            "Epoch [71/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [71/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [71/100], Step [1600/1875], Loss: 0.0383\n",
            "Epoch [71/100], Step [1700/1875], Loss: 0.0022\n",
            "Epoch [71/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [72/100], Step [100/1875], Loss: 0.0070\n",
            "Epoch [72/100], Step [200/1875], Loss: 0.0005\n",
            "Epoch [72/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [72/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [72/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [72/100], Step [600/1875], Loss: 0.0288\n",
            "Epoch [72/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [72/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [72/100], Step [900/1875], Loss: 0.0008\n",
            "Epoch [72/100], Step [1000/1875], Loss: 0.0939\n",
            "Epoch [72/100], Step [1100/1875], Loss: 0.0002\n",
            "Epoch [72/100], Step [1200/1875], Loss: 0.0024\n",
            "Epoch [72/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [72/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [72/100], Step [1500/1875], Loss: 0.0004\n",
            "Epoch [72/100], Step [1600/1875], Loss: 0.1424\n",
            "Epoch [72/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [72/100], Step [1800/1875], Loss: 0.0250\n",
            "Epoch [73/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [73/100], Step [200/1875], Loss: 0.0043\n",
            "Epoch [73/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [73/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [73/100], Step [500/1875], Loss: 0.0003\n",
            "Epoch [73/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [73/100], Step [700/1875], Loss: 0.0142\n",
            "Epoch [73/100], Step [800/1875], Loss: 0.0002\n",
            "Epoch [73/100], Step [900/1875], Loss: 0.2079\n",
            "Epoch [73/100], Step [1000/1875], Loss: 0.0389\n",
            "Epoch [73/100], Step [1100/1875], Loss: 0.0008\n",
            "Epoch [73/100], Step [1200/1875], Loss: 0.0073\n",
            "Epoch [73/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [73/100], Step [1400/1875], Loss: 0.0002\n",
            "Epoch [73/100], Step [1500/1875], Loss: 0.0132\n",
            "Epoch [73/100], Step [1600/1875], Loss: 0.1281\n",
            "Epoch [73/100], Step [1700/1875], Loss: 0.0007\n",
            "Epoch [73/100], Step [1800/1875], Loss: 0.0039\n",
            "Epoch [74/100], Step [100/1875], Loss: 0.0016\n",
            "Epoch [74/100], Step [200/1875], Loss: 0.0007\n",
            "Epoch [74/100], Step [300/1875], Loss: 0.0005\n",
            "Epoch [74/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [74/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [74/100], Step [600/1875], Loss: 0.0006\n",
            "Epoch [74/100], Step [700/1875], Loss: 0.0023\n",
            "Epoch [74/100], Step [800/1875], Loss: 0.0149\n",
            "Epoch [74/100], Step [900/1875], Loss: 0.0967\n",
            "Epoch [74/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [74/100], Step [1100/1875], Loss: 0.0012\n",
            "Epoch [74/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [74/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [74/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [74/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [74/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [74/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [74/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [75/100], Step [100/1875], Loss: 0.0053\n",
            "Epoch [75/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [75/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [600/1875], Loss: 0.0003\n",
            "Epoch [75/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [900/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [1000/1875], Loss: 0.0007\n",
            "Epoch [75/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [75/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [75/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [75/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [1500/1875], Loss: 0.0006\n",
            "Epoch [75/100], Step [1600/1875], Loss: 0.0008\n",
            "Epoch [75/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [75/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [76/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [76/100], Step [200/1875], Loss: 0.0009\n",
            "Epoch [76/100], Step [300/1875], Loss: 0.0003\n",
            "Epoch [76/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [76/100], Step [500/1875], Loss: 0.0004\n",
            "Epoch [76/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [76/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [76/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [76/100], Step [900/1875], Loss: 0.0028\n",
            "Epoch [76/100], Step [1000/1875], Loss: 0.0005\n",
            "Epoch [76/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [76/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [76/100], Step [1300/1875], Loss: 0.0003\n",
            "Epoch [76/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [76/100], Step [1500/1875], Loss: 0.0213\n",
            "Epoch [76/100], Step [1600/1875], Loss: 0.0593\n",
            "Epoch [76/100], Step [1700/1875], Loss: 0.0002\n",
            "Epoch [76/100], Step [1800/1875], Loss: 0.0003\n",
            "Epoch [77/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [77/100], Step [200/1875], Loss: 0.0062\n",
            "Epoch [77/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [77/100], Step [400/1875], Loss: 0.0008\n",
            "Epoch [77/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [77/100], Step [600/1875], Loss: 0.0002\n",
            "Epoch [77/100], Step [700/1875], Loss: 0.0003\n",
            "Epoch [77/100], Step [800/1875], Loss: 0.0003\n",
            "Epoch [77/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [77/100], Step [1000/1875], Loss: 0.0565\n",
            "Epoch [77/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [77/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [77/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [77/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [77/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [77/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [77/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [77/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [78/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [700/1875], Loss: 0.0006\n",
            "Epoch [78/100], Step [800/1875], Loss: 0.0004\n",
            "Epoch [78/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [78/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [1100/1875], Loss: 0.0002\n",
            "Epoch [78/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [78/100], Step [1300/1875], Loss: 0.0088\n",
            "Epoch [78/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [78/100], Step [1500/1875], Loss: 0.0141\n",
            "Epoch [78/100], Step [1600/1875], Loss: 0.0008\n",
            "Epoch [78/100], Step [1700/1875], Loss: 0.0156\n",
            "Epoch [78/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [79/100], Step [100/1875], Loss: 0.0045\n",
            "Epoch [79/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [79/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [79/100], Step [400/1875], Loss: 0.0030\n",
            "Epoch [79/100], Step [500/1875], Loss: 0.0008\n",
            "Epoch [79/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [79/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [79/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [79/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [79/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [79/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [79/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [79/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [79/100], Step [1400/1875], Loss: 0.0157\n",
            "Epoch [79/100], Step [1500/1875], Loss: 0.0009\n",
            "Epoch [79/100], Step [1600/1875], Loss: 0.0233\n",
            "Epoch [79/100], Step [1700/1875], Loss: 0.0040\n",
            "Epoch [79/100], Step [1800/1875], Loss: 0.0953\n",
            "Epoch [80/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [80/100], Step [200/1875], Loss: 0.0004\n",
            "Epoch [80/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [80/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [80/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [900/1875], Loss: 0.0002\n",
            "Epoch [80/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [80/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [80/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [80/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [80/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [81/100], Step [100/1875], Loss: 0.0011\n",
            "Epoch [81/100], Step [200/1875], Loss: 0.0082\n",
            "Epoch [81/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [81/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [81/100], Step [500/1875], Loss: 0.3410\n",
            "Epoch [81/100], Step [600/1875], Loss: 0.0002\n",
            "Epoch [81/100], Step [700/1875], Loss: 0.0007\n",
            "Epoch [81/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [81/100], Step [900/1875], Loss: 0.1032\n",
            "Epoch [81/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [81/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [81/100], Step [1200/1875], Loss: 0.0150\n",
            "Epoch [81/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [81/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [81/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [81/100], Step [1600/1875], Loss: 0.0009\n",
            "Epoch [81/100], Step [1700/1875], Loss: 0.0002\n",
            "Epoch [81/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [82/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [82/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [82/100], Step [300/1875], Loss: 0.0196\n",
            "Epoch [82/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [82/100], Step [500/1875], Loss: 0.1678\n",
            "Epoch [82/100], Step [600/1875], Loss: 0.0005\n",
            "Epoch [82/100], Step [700/1875], Loss: 0.0006\n",
            "Epoch [82/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [82/100], Step [900/1875], Loss: 0.0018\n",
            "Epoch [82/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [82/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [82/100], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [82/100], Step [1300/1875], Loss: 0.0010\n",
            "Epoch [82/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [82/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [82/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [82/100], Step [1700/1875], Loss: 0.0016\n",
            "Epoch [82/100], Step [1800/1875], Loss: 0.0003\n",
            "Epoch [83/100], Step [100/1875], Loss: 0.0004\n",
            "Epoch [83/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [83/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [83/100], Step [400/1875], Loss: 0.0006\n",
            "Epoch [83/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [83/100], Step [600/1875], Loss: 0.0003\n",
            "Epoch [83/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [83/100], Step [800/1875], Loss: 0.0009\n",
            "Epoch [83/100], Step [900/1875], Loss: 0.0515\n",
            "Epoch [83/100], Step [1000/1875], Loss: 0.0979\n",
            "Epoch [83/100], Step [1100/1875], Loss: 0.2388\n",
            "Epoch [83/100], Step [1200/1875], Loss: 0.0038\n",
            "Epoch [83/100], Step [1300/1875], Loss: 0.0005\n",
            "Epoch [83/100], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [83/100], Step [1500/1875], Loss: 0.0010\n",
            "Epoch [83/100], Step [1600/1875], Loss: 0.0313\n",
            "Epoch [83/100], Step [1700/1875], Loss: 0.0009\n",
            "Epoch [83/100], Step [1800/1875], Loss: 0.0009\n",
            "Epoch [84/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [84/100], Step [200/1875], Loss: 0.0002\n",
            "Epoch [84/100], Step [300/1875], Loss: 0.0043\n",
            "Epoch [84/100], Step [400/1875], Loss: 0.0004\n",
            "Epoch [84/100], Step [500/1875], Loss: 0.1097\n",
            "Epoch [84/100], Step [600/1875], Loss: 0.1722\n",
            "Epoch [84/100], Step [700/1875], Loss: 0.0008\n",
            "Epoch [84/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [84/100], Step [900/1875], Loss: 0.0005\n",
            "Epoch [84/100], Step [1000/1875], Loss: 0.0017\n",
            "Epoch [84/100], Step [1100/1875], Loss: 0.0005\n",
            "Epoch [84/100], Step [1200/1875], Loss: 0.0003\n",
            "Epoch [84/100], Step [1300/1875], Loss: 0.0007\n",
            "Epoch [84/100], Step [1400/1875], Loss: 0.0003\n",
            "Epoch [84/100], Step [1500/1875], Loss: 0.0112\n",
            "Epoch [84/100], Step [1600/1875], Loss: 0.0002\n",
            "Epoch [84/100], Step [1700/1875], Loss: 0.0003\n",
            "Epoch [84/100], Step [1800/1875], Loss: 0.0194\n",
            "Epoch [85/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [85/100], Step [200/1875], Loss: 0.0036\n",
            "Epoch [85/100], Step [300/1875], Loss: 0.0004\n",
            "Epoch [85/100], Step [400/1875], Loss: 0.0004\n",
            "Epoch [85/100], Step [500/1875], Loss: 0.0005\n",
            "Epoch [85/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [85/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [85/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [85/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [85/100], Step [1000/1875], Loss: 0.0002\n",
            "Epoch [85/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [85/100], Step [1200/1875], Loss: 0.0129\n",
            "Epoch [85/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [85/100], Step [1400/1875], Loss: 0.0012\n",
            "Epoch [85/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [85/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [85/100], Step [1700/1875], Loss: 0.0010\n",
            "Epoch [85/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [86/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [86/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [86/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [900/1875], Loss: 0.0772\n",
            "Epoch [86/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [86/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [86/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [86/100], Step [1600/1875], Loss: 0.0002\n",
            "Epoch [86/100], Step [1700/1875], Loss: 0.0020\n",
            "Epoch [86/100], Step [1800/1875], Loss: 0.0021\n",
            "Epoch [87/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [87/100], Step [200/1875], Loss: 0.0001\n",
            "Epoch [87/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [87/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [87/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [87/100], Step [600/1875], Loss: 0.0030\n",
            "Epoch [87/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [87/100], Step [800/1875], Loss: 0.0059\n",
            "Epoch [87/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [87/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [87/100], Step [1100/1875], Loss: 0.0003\n",
            "Epoch [87/100], Step [1200/1875], Loss: 0.0002\n",
            "Epoch [87/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [87/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [87/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [87/100], Step [1600/1875], Loss: 0.0031\n",
            "Epoch [87/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [87/100], Step [1800/1875], Loss: 0.1437\n",
            "Epoch [88/100], Step [100/1875], Loss: 0.0003\n",
            "Epoch [88/100], Step [200/1875], Loss: 0.0023\n",
            "Epoch [88/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [600/1875], Loss: 0.0004\n",
            "Epoch [88/100], Step [700/1875], Loss: 0.0007\n",
            "Epoch [88/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [88/100], Step [1000/1875], Loss: 0.0001\n",
            "Epoch [88/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [1200/1875], Loss: 0.0001\n",
            "Epoch [88/100], Step [1300/1875], Loss: 0.0002\n",
            "Epoch [88/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [88/100], Step [1600/1875], Loss: 0.0004\n",
            "Epoch [88/100], Step [1700/1875], Loss: 0.0004\n",
            "Epoch [88/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [200/1875], Loss: 0.0088\n",
            "Epoch [89/100], Step [300/1875], Loss: 0.0087\n",
            "Epoch [89/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [800/1875], Loss: 0.0067\n",
            "Epoch [89/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [89/100], Step [1000/1875], Loss: 0.0005\n",
            "Epoch [89/100], Step [1100/1875], Loss: 0.0011\n",
            "Epoch [89/100], Step [1200/1875], Loss: 0.0949\n",
            "Epoch [89/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [89/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [90/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [90/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [90/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [90/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [90/100], Step [500/1875], Loss: 0.0103\n",
            "Epoch [90/100], Step [600/1875], Loss: 0.2032\n",
            "Epoch [90/100], Step [700/1875], Loss: 0.0004\n",
            "Epoch [90/100], Step [800/1875], Loss: 0.0013\n",
            "Epoch [90/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [90/100], Step [1000/1875], Loss: 0.0013\n",
            "Epoch [90/100], Step [1100/1875], Loss: 0.0008\n",
            "Epoch [90/100], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [90/100], Step [1300/1875], Loss: 0.0090\n",
            "Epoch [90/100], Step [1400/1875], Loss: 0.0002\n",
            "Epoch [90/100], Step [1500/1875], Loss: 0.0019\n",
            "Epoch [90/100], Step [1600/1875], Loss: 0.0005\n",
            "Epoch [90/100], Step [1700/1875], Loss: 0.0003\n",
            "Epoch [90/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [91/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [91/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [300/1875], Loss: 0.0002\n",
            "Epoch [91/100], Step [400/1875], Loss: 0.0005\n",
            "Epoch [91/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [91/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [900/1875], Loss: 0.0001\n",
            "Epoch [91/100], Step [1000/1875], Loss: 0.0060\n",
            "Epoch [91/100], Step [1100/1875], Loss: 0.0005\n",
            "Epoch [91/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [91/100], Step [1600/1875], Loss: 0.0025\n",
            "Epoch [91/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [91/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [92/100], Step [100/1875], Loss: 0.0017\n",
            "Epoch [92/100], Step [200/1875], Loss: 0.0148\n",
            "Epoch [92/100], Step [300/1875], Loss: 0.0027\n",
            "Epoch [92/100], Step [400/1875], Loss: 0.0533\n",
            "Epoch [92/100], Step [500/1875], Loss: 0.0045\n",
            "Epoch [92/100], Step [600/1875], Loss: 0.0010\n",
            "Epoch [92/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [92/100], Step [800/1875], Loss: 0.0014\n",
            "Epoch [92/100], Step [900/1875], Loss: 0.0002\n",
            "Epoch [92/100], Step [1000/1875], Loss: 0.0004\n",
            "Epoch [92/100], Step [1100/1875], Loss: 0.0019\n",
            "Epoch [92/100], Step [1200/1875], Loss: 0.0052\n",
            "Epoch [92/100], Step [1300/1875], Loss: 0.0679\n",
            "Epoch [92/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [92/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [92/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [92/100], Step [1700/1875], Loss: 0.0018\n",
            "Epoch [92/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [93/100], Step [100/1875], Loss: 0.0135\n",
            "Epoch [93/100], Step [200/1875], Loss: 0.0016\n",
            "Epoch [93/100], Step [300/1875], Loss: 0.0524\n",
            "Epoch [93/100], Step [400/1875], Loss: 0.0088\n",
            "Epoch [93/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [93/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [93/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [93/100], Step [800/1875], Loss: 0.0002\n",
            "Epoch [93/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [93/100], Step [1000/1875], Loss: 0.1021\n",
            "Epoch [93/100], Step [1100/1875], Loss: 0.0000\n",
            "Epoch [93/100], Step [1200/1875], Loss: 0.0004\n",
            "Epoch [93/100], Step [1300/1875], Loss: 0.0001\n",
            "Epoch [93/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [93/100], Step [1500/1875], Loss: 0.0011\n",
            "Epoch [93/100], Step [1600/1875], Loss: 0.0005\n",
            "Epoch [93/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [93/100], Step [1800/1875], Loss: 0.0178\n",
            "Epoch [94/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [94/100], Step [200/1875], Loss: 0.0020\n",
            "Epoch [94/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [94/100], Step [400/1875], Loss: 0.0001\n",
            "Epoch [94/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [94/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [94/100], Step [700/1875], Loss: 0.1557\n",
            "Epoch [94/100], Step [800/1875], Loss: 0.0053\n",
            "Epoch [94/100], Step [900/1875], Loss: 0.0007\n",
            "Epoch [94/100], Step [1000/1875], Loss: 0.0002\n",
            "Epoch [94/100], Step [1100/1875], Loss: 0.0042\n",
            "Epoch [94/100], Step [1200/1875], Loss: 0.0008\n",
            "Epoch [94/100], Step [1300/1875], Loss: 0.0005\n",
            "Epoch [94/100], Step [1400/1875], Loss: 0.0002\n",
            "Epoch [94/100], Step [1500/1875], Loss: 0.0080\n",
            "Epoch [94/100], Step [1600/1875], Loss: 0.0005\n",
            "Epoch [94/100], Step [1700/1875], Loss: 0.0039\n",
            "Epoch [94/100], Step [1800/1875], Loss: 0.0005\n",
            "Epoch [95/100], Step [100/1875], Loss: 0.0019\n",
            "Epoch [95/100], Step [200/1875], Loss: 0.0004\n",
            "Epoch [95/100], Step [300/1875], Loss: 0.0003\n",
            "Epoch [95/100], Step [400/1875], Loss: 0.1904\n",
            "Epoch [95/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [95/100], Step [600/1875], Loss: 0.0015\n",
            "Epoch [95/100], Step [700/1875], Loss: 0.0488\n",
            "Epoch [95/100], Step [800/1875], Loss: 0.0035\n",
            "Epoch [95/100], Step [900/1875], Loss: 0.0003\n",
            "Epoch [95/100], Step [1000/1875], Loss: 0.0005\n",
            "Epoch [95/100], Step [1100/1875], Loss: 0.0004\n",
            "Epoch [95/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [95/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [95/100], Step [1400/1875], Loss: 0.0011\n",
            "Epoch [95/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [95/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [95/100], Step [1700/1875], Loss: 0.0001\n",
            "Epoch [95/100], Step [1800/1875], Loss: 0.0002\n",
            "Epoch [96/100], Step [100/1875], Loss: 0.0063\n",
            "Epoch [96/100], Step [200/1875], Loss: 0.0010\n",
            "Epoch [96/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [96/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [96/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [96/100], Step [600/1875], Loss: 0.0017\n",
            "Epoch [96/100], Step [700/1875], Loss: 0.0001\n",
            "Epoch [96/100], Step [800/1875], Loss: 0.0013\n",
            "Epoch [96/100], Step [900/1875], Loss: 0.0002\n",
            "Epoch [96/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [96/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [96/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [96/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [96/100], Step [1400/1875], Loss: 0.0005\n",
            "Epoch [96/100], Step [1500/1875], Loss: 0.0002\n",
            "Epoch [96/100], Step [1600/1875], Loss: 0.2541\n",
            "Epoch [96/100], Step [1700/1875], Loss: 0.0121\n",
            "Epoch [96/100], Step [1800/1875], Loss: 0.0007\n",
            "Epoch [97/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [200/1875], Loss: 0.0002\n",
            "Epoch [97/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [97/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [500/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [700/1875], Loss: 0.0061\n",
            "Epoch [97/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [900/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [1000/1875], Loss: 0.1338\n",
            "Epoch [97/100], Step [1100/1875], Loss: 0.0004\n",
            "Epoch [97/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [1400/1875], Loss: 0.0002\n",
            "Epoch [97/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [1600/1875], Loss: 0.0000\n",
            "Epoch [97/100], Step [1700/1875], Loss: 0.0004\n",
            "Epoch [97/100], Step [1800/1875], Loss: 0.0006\n",
            "Epoch [98/100], Step [100/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [300/1875], Loss: 0.0019\n",
            "Epoch [98/100], Step [400/1875], Loss: 0.0002\n",
            "Epoch [98/100], Step [500/1875], Loss: 0.0010\n",
            "Epoch [98/100], Step [600/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [700/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [800/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [900/1875], Loss: 0.0006\n",
            "Epoch [98/100], Step [1000/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [1100/1875], Loss: 0.0004\n",
            "Epoch [98/100], Step [1200/1875], Loss: 0.0002\n",
            "Epoch [98/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [98/100], Step [1400/1875], Loss: 0.0001\n",
            "Epoch [98/100], Step [1500/1875], Loss: 0.0015\n",
            "Epoch [98/100], Step [1600/1875], Loss: 0.0001\n",
            "Epoch [98/100], Step [1700/1875], Loss: 0.1401\n",
            "Epoch [98/100], Step [1800/1875], Loss: 0.0001\n",
            "Epoch [99/100], Step [100/1875], Loss: 0.0002\n",
            "Epoch [99/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [99/100], Step [300/1875], Loss: 0.0000\n",
            "Epoch [99/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [99/100], Step [500/1875], Loss: 0.0002\n",
            "Epoch [99/100], Step [600/1875], Loss: 0.0009\n",
            "Epoch [99/100], Step [700/1875], Loss: 0.0012\n",
            "Epoch [99/100], Step [800/1875], Loss: 0.0001\n",
            "Epoch [99/100], Step [900/1875], Loss: 0.0080\n",
            "Epoch [99/100], Step [1000/1875], Loss: 0.0971\n",
            "Epoch [99/100], Step [1100/1875], Loss: 0.0006\n",
            "Epoch [99/100], Step [1200/1875], Loss: 0.0008\n",
            "Epoch [99/100], Step [1300/1875], Loss: 0.0004\n",
            "Epoch [99/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [99/100], Step [1500/1875], Loss: 0.0001\n",
            "Epoch [99/100], Step [1600/1875], Loss: 0.0003\n",
            "Epoch [99/100], Step [1700/1875], Loss: 0.0469\n",
            "Epoch [99/100], Step [1800/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [100/1875], Loss: 0.0001\n",
            "Epoch [100/100], Step [200/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [300/1875], Loss: 0.0001\n",
            "Epoch [100/100], Step [400/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [500/1875], Loss: 0.0001\n",
            "Epoch [100/100], Step [600/1875], Loss: 0.0001\n",
            "Epoch [100/100], Step [700/1875], Loss: 0.0002\n",
            "Epoch [100/100], Step [800/1875], Loss: 0.0019\n",
            "Epoch [100/100], Step [900/1875], Loss: 0.0005\n",
            "Epoch [100/100], Step [1000/1875], Loss: 0.0025\n",
            "Epoch [100/100], Step [1100/1875], Loss: 0.0001\n",
            "Epoch [100/100], Step [1200/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [1300/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [1400/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [1500/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [1600/1875], Loss: 0.0002\n",
            "Epoch [100/100], Step [1700/1875], Loss: 0.0000\n",
            "Epoch [100/100], Step [1800/1875], Loss: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oBHQBtNUjeTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "9443db43-15ad-4dd4-be61-d29272167e21"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fee018557f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFKCAYAAAA0WNeQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fed61f3c9e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ddQN8nMgKRb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "  # Test the model\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "      images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4G-JQ4zvLfWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d448fc5-b09b-43c7-e4dc-83597feec319"
      },
      "cell_type": "code",
      "source": [
        "test(model, test_loader)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 99.03 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxKnKTlQjq-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2c3d019b-a030-4e2b-8609-5f2b1de297be"
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for images, labels in test_loader:\n",
        "    images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "    labels = labels.to(device)\n",
        "    print(labels)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    print(predicted)\n",
        "    break"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
            "        4, 0, 7, 4, 0, 1, 3, 1], device='cuda:0')\n",
            "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
            "        4, 0, 7, 4, 0, 1, 3, 1], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zE7CgGPY2-pi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}